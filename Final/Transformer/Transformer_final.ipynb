{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a61b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNIVERSAL TRANSFORMER ATTENTION HEAD PRUNING + MASKED FINE-TUNING\n",
    "(FINAL, GUARANTEED-RUNNING VERSION)\n",
    "\n",
    "✔ Any CSV or JSON Lines dataset\n",
    "✔ Label = last column fallback\n",
    "✔ Labels aligned to model output\n",
    "✔ Binary + multiclass supported\n",
    "✔ Fresh TextVectorization\n",
    "✔ Soft attention-head pruning\n",
    "✔ Masked fine-tuning (graph-safe)\n",
    "✔ Correct FLOPs / Effective FLOPs\n",
    "✔ Handles nested Transformer blocks\n",
    "✔ Legacy .h5 compatible\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================================\n",
    "# IMPORTS\n",
    "# ==========================================================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIG\n",
    "# ==========================================================\n",
    "\n",
    "MAX_LEN     = 200\n",
    "VOCAB_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 5\n",
    "KEEP_RATIO = 0.7\n",
    "\n",
    "# ==========================================================\n",
    "# CUSTOM LAYERS (LEGACY SAFE)\n",
    "# ==========================================================\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_len=None,\n",
    "        vocab_size=None,\n",
    "        embed_dim=None,\n",
    "        maxlen=None,          # legacy support\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if max_len is None and maxlen is not None:\n",
    "            max_len = maxlen\n",
    "\n",
    "        if max_len is None:\n",
    "            raise ValueError(\"max_len or maxlen must be provided\")\n",
    "\n",
    "        self.max_len = int(max_len)\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.embed_dim = int(embed_dim)\n",
    "\n",
    "        self.token_emb = layers.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.pos_emb   = layers.Embedding(self.max_len, self.embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        pos = tf.range(start=0, limit=tf.shape(x)[-1])\n",
    "        return self.token_emb(x) + self.pos_emb(pos)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"maxlen\": self.max_len,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.drop1 = layers.Dropout(rate)\n",
    "        self.drop2 = layers.Dropout(rate)\n",
    "\n",
    "        self.last_attn = None\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        attn = self.att(x, x, training=training)\n",
    "        self.last_attn = attn\n",
    "        x = self.ln1(x + self.drop1(attn, training=training))\n",
    "        ffn = self.ffn(x, training=training)\n",
    "        return self.ln2(x + self.drop2(ffn, training=training))\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "\n",
    "TransformerBlock = TransformerEncoder\n",
    "\n",
    "# ==========================================================\n",
    "# MASKED TRANSFORMER\n",
    "# ==========================================================\n",
    "\n",
    "class MaskedTransformer(tf.keras.Model):\n",
    "    def __init__(self, base_model, masks):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.masks = masks\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        for layer in self.base.layers:\n",
    "            if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "                continue\n",
    "\n",
    "            if isinstance(layer, TransformerEncoder):\n",
    "                attn = layer.att(x, x, training=training)\n",
    "\n",
    "                if layer in self.masks:\n",
    "                    H = layer.att.num_heads\n",
    "                    D = attn.shape[-1]\n",
    "                    Hd = D // H\n",
    "                    m = self.masks[layer]\n",
    "\n",
    "                    attn = tf.reshape(attn, (-1, tf.shape(attn)[1], H, Hd))\n",
    "                    attn = attn * m[None, None, :, None]\n",
    "                    attn = tf.reshape(attn, (-1, tf.shape(attn)[1], D))\n",
    "\n",
    "                x = layer.ln1(x + layer.drop1(attn, training=training))\n",
    "                x = layer.ln2(x + layer.drop2(layer.ffn(x), training=training))\n",
    "            else:\n",
    "                x = layer(x, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "# ==========================================================\n",
    "# DATASET LOADER (ALIGNED TO MODEL)\n",
    "# ==========================================================\n",
    "\n",
    "def load_text_dataset(path, model_num_outputs):\n",
    "\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    elif path.endswith(\".json\"):\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                rows.append(json.loads(line))\n",
    "        df = pd.DataFrame(rows)\n",
    "    else:\n",
    "        raise ValueError(\"Only CSV and JSON supported\")\n",
    "\n",
    "    print(\"[INFO] Dataset columns:\", list(df.columns))\n",
    "\n",
    "    text_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    texts = df[text_cols].fillna(\"\").agg(\" \".join, axis=1).values\n",
    "\n",
    "    label_col = df.columns[-1]\n",
    "    print(f\"[INFO] Using label column: {label_col}\")\n",
    "\n",
    "    labels = df[label_col].values\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(labels):\n",
    "        y_int = labels.astype(int)\n",
    "    else:\n",
    "        y_int = LabelEncoder().fit_transform(labels)\n",
    "\n",
    "    # ======================================================\n",
    "    # CRITICAL: BINARY vs MULTICLASS HANDLING\n",
    "    # ======================================================\n",
    "\n",
    "    if model_num_outputs == 1:\n",
    "        # Dense(1) → sigmoid → binary\n",
    "        y = y_int.astype(\"float32\")\n",
    "        loss = \"binary_crossentropy\"\n",
    "\n",
    "    else:\n",
    "        # Dense(N) → softmax\n",
    "        y_int = np.clip(y_int, 0, model_num_outputs - 1)\n",
    "        y = to_categorical(y_int, num_classes=model_num_outputs)\n",
    "        loss = \"categorical_crossentropy\"\n",
    "\n",
    "    return texts, y, loss\n",
    "\n",
    "# ==========================================================\n",
    "# TEXT VECTORIZATION\n",
    "# ==========================================================\n",
    "\n",
    "def build_vectorizer(texts):\n",
    "    vec = layers.TextVectorization(\n",
    "        max_tokens=VOCAB_SIZE,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=MAX_LEN\n",
    "    )\n",
    "    vec.adapt(texts)\n",
    "    return vec\n",
    "\n",
    "# ==========================================================\n",
    "# STRIP TRANSFORMER\n",
    "# ==========================================================\n",
    "\n",
    "def strip_transformer(model):\n",
    "    if model.input.dtype in (tf.int32, tf.int64):\n",
    "        return model\n",
    "\n",
    "    for layer in model.layers:\n",
    "        try:\n",
    "            if layer.input.dtype in (tf.int32, tf.int64):\n",
    "                return tf.keras.Model(layer.input, model.output)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    token_input = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
    "    return tf.keras.Model(token_input, model(token_input))\n",
    "\n",
    "# ==========================================================\n",
    "# UTILITIES\n",
    "# ==========================================================\n",
    "\n",
    "def get_all_layers(model):\n",
    "    out = []\n",
    "    for l in model.layers:\n",
    "        out.append(l)\n",
    "        if isinstance(l, tf.keras.Model):\n",
    "            out.extend(get_all_layers(l))\n",
    "    return out\n",
    "\n",
    "# ==========================================================\n",
    "# ATTENTION STATS + FLOPs\n",
    "# ==========================================================\n",
    "\n",
    "def compute_attention_head_stats(model, vectorizer, texts, max_batches=20):\n",
    "    blocks = [l for l in get_all_layers(model) if isinstance(l, TransformerEncoder)]\n",
    "    stats = {b: [] for b in blocks}\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(texts).batch(BATCH_SIZE)\n",
    "    for i, xb in enumerate(ds):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "\n",
    "        tokens = tf.cast(vectorizer(xb), tf.int32)\n",
    "        _ = model(tokens, training=False)\n",
    "\n",
    "        for block in blocks:\n",
    "            if block.last_attn is None:\n",
    "                continue\n",
    "            attn = block.last_attn\n",
    "            H = block.att.num_heads\n",
    "            D = attn.shape[-1]\n",
    "            Hd = D // H\n",
    "            reshaped = tf.reshape(attn, (-1, attn.shape[1], H, Hd))\n",
    "            score = tf.reduce_mean(tf.abs(reshaped), axis=[0, 1, 3]).numpy()\n",
    "            stats[block].append(score)\n",
    "\n",
    "    return {k: np.mean(v, axis=0) for k, v in stats.items()}\n",
    "\n",
    "\n",
    "def compute_importance_mask(stats, keep_ratio):\n",
    "    masks = {}\n",
    "    for block, score in stats.items():\n",
    "        k = max(1, int(len(score) * keep_ratio))\n",
    "        th = np.partition(score, -k)[-k]\n",
    "        masks[block] = (score >= th).astype(np.float32)\n",
    "    return masks\n",
    "\n",
    "\n",
    "def attention_flops(seq_len, embed_dim):\n",
    "    return 4 * seq_len * embed_dim * embed_dim + 2 * seq_len * seq_len * embed_dim\n",
    "\n",
    "\n",
    "def transformer_model_flops(model):\n",
    "    flops = 0\n",
    "    for l in get_all_layers(model):\n",
    "        if isinstance(l, TransformerEncoder):\n",
    "            D = l.att.key_dim * l.att.num_heads\n",
    "            flops += attention_flops(MAX_LEN, D)\n",
    "    return flops\n",
    "\n",
    "\n",
    "def effective_transformer_flops(model, masks):\n",
    "    flops = 0\n",
    "    for l in get_all_layers(model):\n",
    "        if isinstance(l, TransformerEncoder):\n",
    "            D = l.att.key_dim * l.att.num_heads\n",
    "            H = l.att.num_heads\n",
    "            kept = int(np.sum(masks.get(l, np.ones(H))))\n",
    "            flops += attention_flops(MAX_LEN, D) * (kept / H)\n",
    "    return flops\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN PIPELINE\n",
    "# ==========================================================\n",
    "\n",
    "def universal_transformer_pruning(model_path, dataset_path):\n",
    "\n",
    "    model = tf.keras.models.load_model(\n",
    "        model_path,\n",
    "        custom_objects={\n",
    "            \"PositionalEmbedding\": PositionalEmbedding,\n",
    "            \"TransformerEncoder\": TransformerEncoder,\n",
    "            \"TransformerBlock\": TransformerEncoder,\n",
    "        },\n",
    "        compile=False\n",
    "    )\n",
    "\n",
    "    model_num_outputs = model.output_shape[-1]\n",
    "    print(f\"[INFO] Model output units: {model_num_outputs}\")\n",
    "\n",
    "    texts, y, loss_fn = load_text_dataset(dataset_path, model_num_outputs)\n",
    "\n",
    "    Xtr, Xv, ytr, yv = train_test_split(\n",
    "        texts, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y if model_num_outputs == 1 else y.argmax(axis=1)\n",
    "    )\n",
    "\n",
    "    vectorizer = build_vectorizer(Xtr)\n",
    "    transformer = strip_transformer(model)\n",
    "\n",
    "    stats = compute_attention_head_stats(transformer, vectorizer, Xtr)\n",
    "    masks = compute_importance_mask(stats, KEEP_RATIO)\n",
    "\n",
    "    masked_model = MaskedTransformer(transformer, masks)\n",
    "    masked_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=loss_fn,\n",
    "        metrics=[\"accuracy\"],\n",
    "        run_eagerly=True\n",
    "    )\n",
    "\n",
    "    masked_model.fit(\n",
    "        tf.cast(vectorizer(Xtr), tf.int32), ytr,\n",
    "        validation_data=(tf.cast(vectorizer(Xv), tf.int32), yv),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    acc = masked_model.evaluate(\n",
    "        tf.cast(vectorizer(Xv), tf.int32), yv, verbose=0\n",
    "    )[1]\n",
    "\n",
    "    orig = transformer_model_flops(transformer)\n",
    "    eff  = effective_transformer_flops(transformer, masks)\n",
    "\n",
    "    print(\"\\n=========== FINAL RESULTS ===========\")\n",
    "    print(f\"Masked Accuracy     : {acc:.4f}\")\n",
    "    print(f\"Original GFLOPs     : {orig / 1e9:.3f}\")\n",
    "    print(f\"Effective GFLOPs    : {eff / 1e9:.3f}\")\n",
    "    print(f\"FLOPs Reduction (%) : {(1 - eff / orig) * 100:.2f}%\")\n",
    "\n",
    "    masked_model.save(\"soft_pruned_transformer_imbd.keras\")\n",
    "    print(\"[INFO] Saved pruned model: soft_pruned_transformer.keras\")\n",
    "\n",
    "# ==========================================================\n",
    "# RUN\n",
    "# ==========================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    universal_transformer_pruning(\n",
    "        r\"D:\\college\\sem-8\\final\\custom_transformer_imdb.h5\",\n",
    "        r\"D:\\college\\sem-8\\dataset\\tranformer dataset\\IMDB Dataset.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
