{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3e15b0",
   "metadata": {},
   "source": [
    "# **Main FNN Pruned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64512aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FNN-only pruning pipeline (auto-detect model type).\n",
    "- Detects model type from final layer (binary / multiclass / regression).\n",
    "- Adapts CSV loader accordingly.\n",
    "- Keeps final layer intact (never pruned).\n",
    "- Reports FLOPs, GFLOPs, MFLOPS and inference time.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# Helper: detect model type\n",
    "# -----------------------\n",
    "def detect_model_type(model):\n",
    "    out = model.layers[-1]\n",
    "    units = getattr(out, \"units\", None)\n",
    "    act = getattr(out, \"activation\", None)\n",
    "    act_name = act.__name__ if act is not None else None\n",
    "\n",
    "    if units == 1 and act_name in (\"sigmoid\",):\n",
    "        return \"binary\"\n",
    "    if units == 1 and act_name in (\"linear\", \"relu\", \"tanh\"):\n",
    "        return \"regression\"\n",
    "    if units is not None and units > 1 and act_name == \"softmax\":\n",
    "        return \"multiclass\"\n",
    "    # fallback tries\n",
    "    if units == 1:\n",
    "        return \"binary\"\n",
    "    if units and units > 1:\n",
    "        return \"multiclass\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def get_loss_and_metrics_for_type(model_type):\n",
    "    if model_type == \"binary\":\n",
    "        return (\"binary_crossentropy\", [\"accuracy\"])\n",
    "    if model_type == \"multiclass\":\n",
    "        # use sparse labels to avoid forcing one-hot encoding\n",
    "        return (\"sparse_categorical_crossentropy\", [\"sparse_categorical_accuracy\"])\n",
    "    if model_type == \"regression\":\n",
    "        return (\"mse\", [\"mse\"])\n",
    "    raise ValueError(\"Unknown model type\")\n",
    "\n",
    "# =====================================================================\n",
    "# 1) UNIVERSAL CSV LOADER  (adapts target based on model_type)\n",
    "# =====================================================================\n",
    "def load_any_csv_dataset(csv_path, target=None, test_size=0.2, val_size=0.1, model_type=\"binary\"):\n",
    "    \"\"\"\n",
    "    Loads CSV (auto-detect delimiter). Adapts target processing to model_type:\n",
    "      - binary/regression: returns y as float32 shaped (n,1)\n",
    "      - multiclass: returns integer labels shaped (n,1) for sparse loss\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Loading CSV with automatic delimiter detection...\")\n",
    "    df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "    print(\"[INFO] Columns detected:\", list(df.columns))\n",
    "\n",
    "    if target is None:\n",
    "        target = df.columns[-1]\n",
    "        print(f\"[INFO] Auto-detected target column = {target}\")\n",
    "\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found in dataset!\")\n",
    "\n",
    "    y = df[target]\n",
    "    X = df.drop(columns=[target])\n",
    "\n",
    "    # Preprocess target based on model_type\n",
    "    if model_type == \"binary\":\n",
    "        # convert yes/no/true/false -> 1/0\n",
    "        if y.dtype == object:\n",
    "            y = y.astype(str).str.strip().str.lower()\n",
    "            y = y.replace({\"yes\": 1, \"no\": 0, \"true\": 1, \"false\": 0})\n",
    "        # ensure numeric\n",
    "        y = y.astype(np.float32).values.reshape(-1, 1)\n",
    "\n",
    "    elif model_type == \"multiclass\":\n",
    "        # convert strings to categorical integer codes if needed\n",
    "        if y.dtype == object:\n",
    "            y = y.astype(\"category\").cat.codes\n",
    "        # ensure integer labels (sparse labels)\n",
    "        y = y.astype(np.int32).values.reshape(-1, 1)\n",
    "\n",
    "    elif model_type == \"regression\":\n",
    "        # numeric continuous\n",
    "        y = y.astype(np.float32).values.reshape(-1, 1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_type for loader\")\n",
    "\n",
    "    # One-hot encode only categorical predictors (keep numeric as-is)\n",
    "    cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "    if len(cat_cols) > 0:\n",
    "        print(\"[INFO] One-hot encoding predictors:\", list(cat_cols))\n",
    "        X = pd.get_dummies(X, drop_first=True)\n",
    "    else:\n",
    "        print(\"[INFO] No categorical predictor columns to encode.\")\n",
    "\n",
    "    # convert to float32 (features)\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=(test_size + val_size), random_state=42, stratify=y if model_type!=\"regression\" else None\n",
    "    )\n",
    "    # second split into val/test\n",
    "    val_ratio = val_size / (test_size + val_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=(1 - val_ratio), random_state=42,\n",
    "        stratify=y_temp if model_type!=\"regression\" else None\n",
    "    )\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "# =====================================================================\n",
    "# 2) GET DENSE LAYERS\n",
    "# =====================================================================\n",
    "def get_dense_layers(model):\n",
    "    return [layer for layer in model.layers if isinstance(layer, Dense)]\n",
    "\n",
    "# =====================================================================\n",
    "# 3) ACTIVATION + GRADIENT STATS  (SHAPE-SAFE)\n",
    "# =====================================================================\n",
    "def compute_activation_grad_stats(model, dense_layers, X, y, model_type,\n",
    "                                  batch_size=64, max_batches=30):\n",
    "    \"\"\"\n",
    "    Returns stats dict: {layer.name: (A_mean_per_neuron, G_mean_per_neuron, Var_per_neuron)}\n",
    "    Uses appropriate loss for model_type when computing gradients.\n",
    "    \"\"\"\n",
    "    # Convert X,y to numpy if DataFrame\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_np = X.to_numpy()\n",
    "    else:\n",
    "        X_np = X\n",
    "    if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "        y_np = y.to_numpy()\n",
    "    else:\n",
    "        y_np = y\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_np, y_np)).batch(batch_size)\n",
    "    acc_A = {l.name: [] for l in dense_layers}\n",
    "    acc_G = {l.name: [] for l in dense_layers}\n",
    "    acc_V = {l.name: [] for l in dense_layers}\n",
    "\n",
    "    # choose loss for gradient computation\n",
    "    if model_type == \"binary\":\n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n",
    "    elif model_type == \"multiclass\":\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n",
    "    elif model_type == \"regression\":\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model_type for stats\")\n",
    "\n",
    "    batch_count = 0\n",
    "    for xb, yb in ds:\n",
    "        if batch_count >= max_batches:\n",
    "            break\n",
    "        batch_count += 1\n",
    "\n",
    "        # ensure shapes consistent\n",
    "        x = tf.cast(xb, tf.float32)\n",
    "        # For sparse multiclass loss, yb should be shape (batch,) ints\n",
    "        if model_type == \"multiclass\":\n",
    "            yb_proc = tf.cast(tf.squeeze(yb, axis=-1), tf.int32)\n",
    "        else:\n",
    "            # binary/regression: keep shape (batch,1) as float32\n",
    "            yb_proc = tf.cast(yb, tf.float32)\n",
    "\n",
    "        layer_outputs = {}\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            for layer in model.layers:\n",
    "                x = layer(x)\n",
    "                if layer in dense_layers:\n",
    "                    tape.watch(x)\n",
    "                    layer_outputs[layer.name] = x\n",
    "            preds = x\n",
    "            # compute per-sample loss (shape [batch,] or [batch, ...])\n",
    "            per_sample_loss = loss_fn(yb_proc, preds)\n",
    "            # compute scalar loss for gradient direction\n",
    "            loss = tf.reduce_mean(per_sample_loss)\n",
    "\n",
    "        for layer in dense_layers:\n",
    "            name = layer.name\n",
    "            if name not in layer_outputs:\n",
    "                continue\n",
    "            a = layer_outputs[name]            # shape (batch, units)\n",
    "            A = tf.reduce_mean(tf.abs(a), axis=0).numpy()\n",
    "            V = tf.math.reduce_variance(a, axis=0).numpy()\n",
    "\n",
    "            g = tape.gradient(loss, a)\n",
    "            if g is None:\n",
    "                G = np.zeros_like(A)\n",
    "            else:\n",
    "                G = tf.reduce_mean(tf.abs(g), axis=0).numpy()\n",
    "\n",
    "            acc_A[name].append(A)\n",
    "            acc_G[name].append(G)\n",
    "            acc_V[name].append(V)\n",
    "\n",
    "        del tape\n",
    "\n",
    "    # Aggregate\n",
    "    stats = {}\n",
    "    for layer in dense_layers:\n",
    "        name = layer.name\n",
    "        if len(acc_A[name]) == 0:\n",
    "            stats[name] = (np.array([]), np.array([]), np.array([]))\n",
    "            continue\n",
    "        A = np.mean(np.stack(acc_A[name], axis=0), axis=0)\n",
    "        G = np.mean(np.stack(acc_G[name], axis=0), axis=0)\n",
    "        V = np.mean(np.stack(acc_V[name], axis=0), axis=0)\n",
    "        stats[name] = (A, G, V)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =====================================================================\n",
    "# 4) IMPORTANCE MASKS (protect last Dense layer)\n",
    "# =====================================================================\n",
    "def compute_importance_mask(\n",
    "        stats,\n",
    "        keep_ratio=0.7,\n",
    "        alpha=0.5,\n",
    "        beta=0.3,\n",
    "        gamma=0.2,\n",
    "        last_dense_name=None,\n",
    "        min_units=4  # <-- increase minimum neurons per layer\n",
    "    ):\n",
    "    \n",
    "    masks = {}\n",
    "    for name, (A, G, V) in stats.items():\n",
    "        if A.size == 0:\n",
    "            masks[name] = np.array([], dtype=np.float32)\n",
    "            continue\n",
    "\n",
    "        def normalize(x):\n",
    "            x2 = x - np.min(x)\n",
    "            mx = np.max(x2)\n",
    "            return x2 / (mx + 1e-12)\n",
    "\n",
    "        # compute score\n",
    "        score = alpha * normalize(A) + beta * normalize(G) + gamma * normalize(V)\n",
    "\n",
    "        # ---- NEVER prune final Dense layer ----\n",
    "        if last_dense_name is not None and name == last_dense_name:\n",
    "            masks[name] = np.ones_like(score, dtype=np.float32)\n",
    "            continue\n",
    "\n",
    "        # ---- compute K ----\n",
    "        k = int(len(score) * keep_ratio)\n",
    "        k = max(k, min_units)        # enforce minimum\n",
    "        k = min(k, len(score))       # safety\n",
    "\n",
    "        # ---- If all scores are zero â†’ randomly pick k neurons ----\n",
    "        if np.all(score == 0):\n",
    "            sel = np.random.choice(len(score), size=k, replace=False)\n",
    "            mask = np.zeros_like(score)\n",
    "            mask[sel] = 1\n",
    "            masks[name] = mask.astype(np.float32)\n",
    "            continue\n",
    "\n",
    "        # normal path\n",
    "        thresh = np.partition(score, -k)[-k]\n",
    "        mask = (score >= thresh).astype(np.float32)\n",
    "\n",
    "        # final safety: ensure >= min_units survive\n",
    "        if np.sum(mask) < min_units:\n",
    "            topk = np.argsort(score)[-min_units:]\n",
    "            mask = np.zeros_like(score)\n",
    "            mask[topk] = 1\n",
    "\n",
    "        masks[name] = mask.astype(np.float32)\n",
    "\n",
    "    return masks\n",
    "\n",
    "# =====================================================================\n",
    "# 5) DNAGate Layer (Zeroing)\n",
    "# =====================================================================\n",
    "# =====================================================================\n",
    "# 5) DNAGate Layer (Zeroing) â€” NOW FULLY SERIALIZABLE\n",
    "# =====================================================================\n",
    "class DNAGate(tf.keras.layers.Layer):\n",
    "    def __init__(self, mask=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # store original numpy mask (for config saving)\n",
    "        self._mask_np = None\n",
    "        if mask is not None:\n",
    "            self._mask_np = np.array(mask, dtype=np.float32)\n",
    "\n",
    "        # keras-serializable field (converted to list)\n",
    "        self.mask_list = self._mask_np.tolist() if self._mask_np is not None else None\n",
    "        self.mask = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.mask_list is None:\n",
    "            self.mask = None\n",
    "        else:\n",
    "            # rebuild tensor mask\n",
    "            mask_np = np.array(self.mask_list, dtype=np.float32)\n",
    "            if input_shape[-1] != mask_np.shape[0]:\n",
    "                raise ValueError(\n",
    "                    f\"DNAGate mask length {mask_np.shape[0]} != layer units {input_shape[-1]}\"\n",
    "                )\n",
    "            self.mask = tf.constant(mask_np, dtype=self.dtype)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.mask is None:\n",
    "            return x\n",
    "        return x * self.mask\n",
    "\n",
    "    # â­â­ IMPORTANT â€” MAKES THE LAYER SAVEABLE â­â­\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"mask\": self.mask_list,   # store mask as list\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # mask list gets restored automatically\n",
    "        return cls(**config)\n",
    "\n",
    "def build_masked_model(orig_model, masks):\n",
    "    inp = Input(shape=orig_model.input_shape[1:])\n",
    "    x = inp\n",
    "    for layer in orig_model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name in masks:\n",
    "            x = DNAGate(mask=masks[layer.name], name=layer.name + \"_dna\")(x)\n",
    "    new_model = Model(inp, x)\n",
    "    # copy weights where possible\n",
    "    for l in orig_model.layers:\n",
    "        try:\n",
    "            new_model.get_layer(l.name).set_weights(l.get_weights())\n",
    "        except Exception:\n",
    "            pass\n",
    "    return new_model\n",
    "\n",
    "# =====================================================================\n",
    "# 6) FLOPS + INFERENCE TIME\n",
    "# =====================================================================\n",
    "def dense_flops(in_size, out_size, bias=True):\n",
    "    # multiply-adds counted as 2 ops per MAC\n",
    "    return int(in_size * out_size * 2 + (out_size if bias else 0))\n",
    "\n",
    "def model_flops(model):\n",
    "    total = 0\n",
    "    in_size = int(model.input_shape[1])\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Dense):\n",
    "            total += dense_flops(in_size, layer.units, layer.use_bias)\n",
    "            in_size = layer.units\n",
    "    return total\n",
    "\n",
    "def effective_flops(model, masks):\n",
    "    total = 0\n",
    "    in_size = int(model.input_shape[1])\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Dense):\n",
    "            if layer.name in masks and masks[layer.name].size > 0:\n",
    "                units = int(np.sum(masks[layer.name]))\n",
    "            else:\n",
    "                units = layer.units\n",
    "            total += dense_flops(in_size, units, layer.use_bias)\n",
    "            in_size = units\n",
    "    return total\n",
    "\n",
    "def flops_to_gflops(f):\n",
    "    return f / 1e9\n",
    "\n",
    "def flops_to_mflops(f):\n",
    "    return f / 1e6\n",
    "\n",
    "def measure_inference_time(model, X, runs=200):\n",
    "    \"\"\"Return average inference time in milliseconds per sample.\n",
    "       Accepts X as pandas DataFrame or numpy array.\"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_np = X.to_numpy()\n",
    "    else:\n",
    "        X_np = X\n",
    "    if len(X_np) == 0:\n",
    "        return float(\"nan\")\n",
    "    # pick random indices\n",
    "    idx = np.random.randint(0, len(X_np), size=min(runs, len(X_np)))\n",
    "    samples = X_np[idx]\n",
    "    # warm-up single predict\n",
    "    model.predict(samples[:1], verbose=0)\n",
    "    t0 = time.time()\n",
    "    model.predict(samples, verbose=0)\n",
    "    t1 = time.time()\n",
    "    total_ms = (t1 - t0) * 1000.0\n",
    "    return total_ms / len(samples)\n",
    "\n",
    "# =====================================================================\n",
    "# 7) STRUCTURAL PRUNING (REMOVE NEURONS) â€” protect final Dense layer\n",
    "# =====================================================================\n",
    "def structurally_prune_fnn(orig_model, masks, last_dense_name=None):\n",
    "    inp = Input(shape=orig_model.input_shape[1:])\n",
    "    x = inp\n",
    "    prev_sel = None\n",
    "\n",
    "    for layer in orig_model.layers:\n",
    "        if isinstance(layer, Dense):\n",
    "            W, b = layer.get_weights()\n",
    "\n",
    "            # If previous layer was pruned â†’ prune rows\n",
    "            if prev_sel is not None:\n",
    "                W = W[prev_sel, :]\n",
    "\n",
    "            # ---- SAFETY: determine neurons to keep ----\n",
    "            if last_dense_name is not None and layer.name == last_dense_name:\n",
    "                # never prune final layer\n",
    "                sel = np.arange(layer.units)\n",
    "\n",
    "            else:\n",
    "                mask = masks.get(layer.name, None)\n",
    "                if mask is not None and mask.size > 0:\n",
    "                    sel = np.where(mask == 1)[0]\n",
    "\n",
    "                    # ðŸ’¥ SAFETY FIX: ensure >=1 neuron survives\n",
    "                    if len(sel) == 0:\n",
    "                        sel = np.array([np.argmax(mask)])  \n",
    "                else:\n",
    "                    sel = None\n",
    "\n",
    "            # ---- Apply pruning ----\n",
    "            if sel is not None:\n",
    "                W_new = W[:, sel]\n",
    "                b_new = b[sel]\n",
    "                new_units = len(sel)\n",
    "            else:\n",
    "                W_new = W\n",
    "                b_new = b\n",
    "                new_units = W.shape[1]\n",
    "\n",
    "            new_layer = Dense(new_units, activation=layer.activation, name=layer.name)\n",
    "            x = new_layer(x)\n",
    "            new_layer.set_weights([W_new, b_new])\n",
    "\n",
    "            prev_sel = sel\n",
    "\n",
    "        else:\n",
    "            x = layer(x)\n",
    "\n",
    "    return Model(inp, x)\n",
    "\n",
    "# =====================================================================\n",
    "# 8) MAIN PIPELINE (auto-detect model type)\n",
    "# =====================================================================\n",
    "def fnn_pruning_pipeline(model_path, dataset_path,\n",
    "                         keep_ratio=0.7,\n",
    "                         alpha=0.5, beta=0.3, gamma=0.2,\n",
    "                         calib_batches=30, batch_size=64, ft_epochs=3):\n",
    "    # 1) load model and detect type\n",
    "    print(\"Loading model:\", model_path)\n",
    "    model = load_model(model_path)\n",
    "    model_type = detect_model_type(model)\n",
    "    print(f\"[INFO] Detected model type: {model_type}\")\n",
    "\n",
    "    loss_name, metrics = get_loss_and_metrics_for_type(model_type)\n",
    "    print(f\"[INFO] Using loss='{loss_name}', metrics={metrics}\")\n",
    "\n",
    "    # 2) load dataset adapted to model_type\n",
    "    print(\"Loading dataset:\", dataset_path)\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_any_csv_dataset(dataset_path, model_type=model_type)\n",
    "\n",
    "    # 3) find dense layers and the last Dense layer name\n",
    "    dense_layers = get_dense_layers(model)\n",
    "    last_dense_name = dense_layers[-1].name if len(dense_layers) > 0 else None\n",
    "    print(\"Dense layers:\", [l.name for l in dense_layers], \"| last_dense:\", last_dense_name)\n",
    "\n",
    "    # 4) compute stats (activation + gradient)\n",
    "    print(\"Computing activation & gradient stats...\")\n",
    "    stats = compute_activation_grad_stats(model, dense_layers, X_train, y_train, model_type,\n",
    "                                          batch_size=batch_size, max_batches=calib_batches)\n",
    "\n",
    "    # 5) compute masks (protect final layer)\n",
    "    masks = compute_importance_mask(stats, keep_ratio=keep_ratio, alpha=alpha, beta=beta, gamma=gamma, last_dense_name=last_dense_name)\n",
    "    for k,v in masks.items():\n",
    "        if v.size > 0:\n",
    "            print(f\"{k} -> kept units: {int(np.sum(v))}/{len(v)}\")\n",
    "\n",
    "    # 6) build masked model (DNAGate zeroing) and compile with correct loss/metrics\n",
    "    print(\"Building masked model (DNAGate zeroing)...\")\n",
    "    masked_model = build_masked_model(model, masks)\n",
    "    masked_model.compile(optimizer='adam', loss=loss_name, metrics=metrics)\n",
    "\n",
    "    # compute baseline & masked flops/accuracy\n",
    "    baseline_acc = None\n",
    "    masked_acc_before = None\n",
    "    if model_type == \"regression\":\n",
    "        baseline_eval = model.evaluate(X_val, y_val, verbose=0)\n",
    "        baseline_acc = baseline_eval[0] if len(baseline_eval)>0 else None\n",
    "        masked_acc_before = masked_model.evaluate(X_val, y_val, verbose=0)[0]\n",
    "    else:\n",
    "        baseline_acc = model.evaluate(X_val, y_val, verbose=0)[1]  # accuracy or sparse_accuracy\n",
    "        masked_acc_before = masked_model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "\n",
    "    baseline_flops = model_flops(model)\n",
    "    masked_flops = effective_flops(model, masks)\n",
    "\n",
    "    print(f\"Baseline Acc: {baseline_acc} | FLOPS: {baseline_flops}\")\n",
    "    print(f\"Masked Acc (before FT): {masked_acc_before} | Effective FLOPS: {masked_flops}\")\n",
    "\n",
    "    # 7) fine-tune masked model\n",
    "    print(\"Fine-tuning masked model...\")\n",
    "    masked_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=ft_epochs, verbose=2)\n",
    "\n",
    "    # evaluate masked after FT\n",
    "    if model_type == \"regression\":\n",
    "        masked_eval = masked_model.evaluate(X_val, y_val, verbose=0)\n",
    "        masked_acc_after = masked_eval[0] if len(masked_eval)>0 else None\n",
    "    else:\n",
    "        masked_acc_after = masked_model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "\n",
    "    # 8) structural pruning (permanent neuron removal)\n",
    "    print(\"Constructing structurally-pruned model (remove neurons)...\")\n",
    "    pruned_model = structurally_prune_fnn(model, masks, last_dense_name=last_dense_name)\n",
    "    pruned_model.compile(optimizer='adam', loss=loss_name, metrics=metrics)\n",
    "\n",
    "    print(\"Fine-tuning pruned model...\")\n",
    "    pruned_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=ft_epochs, verbose=2)\n",
    "\n",
    "    # final evals\n",
    "    if model_type == \"regression\":\n",
    "        pruned_eval = pruned_model.evaluate(X_val, y_val, verbose=0)\n",
    "        pruned_acc = pruned_eval[0] if len(pruned_eval)>0 else None\n",
    "    else:\n",
    "        pruned_acc = pruned_model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "\n",
    "    pruned_flops = model_flops(pruned_model)\n",
    "\n",
    "    # compute GFLOPS / MFLOPS / inference times\n",
    "    baseline_gflops = flops_to_gflops(baseline_flops)\n",
    "    masked_gflops = flops_to_gflops(masked_flops)\n",
    "    pruned_gflops = flops_to_gflops(pruned_flops)\n",
    "\n",
    "    baseline_mflops = flops_to_mflops(baseline_flops)\n",
    "    masked_mflops = flops_to_mflops(masked_flops)\n",
    "    pruned_mflops = flops_to_mflops(pruned_flops)\n",
    "\n",
    "    baseline_time_ms = measure_inference_time(model, X_val)\n",
    "    masked_time_ms = measure_inference_time(masked_model, X_val)\n",
    "    pruned_time_ms = measure_inference_time(pruned_model, X_val)\n",
    "\n",
    "    print(\"\\n=== FINAL REPORT ===\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    print(f\"Baseline:   Acc/metric={baseline_acc} | FLOPS={baseline_flops} | MFLOPS={baseline_mflops:.6f} | GFLOPS={baseline_gflops:.10f} | Time={baseline_time_ms:.4f} ms/sample\")\n",
    "    print(f\"Masked:     Acc/metric={masked_acc_after} | EFLOPS={masked_flops} | MFLOPS={masked_mflops:.6f} | GFLOPS={masked_gflops:.10f} | Time={masked_time_ms:.4f} ms/sample\")\n",
    "    print(f\"Pruned:     Acc/metric={pruned_acc} | FLOPS={pruned_flops} | MFLOPS={pruned_mflops:.6f} | GFLOPS={pruned_gflops:.10f} | Time={pruned_time_ms:.4f} ms/sample\")\n",
    "    if baseline_flops > 0:\n",
    "        print(f\"FLOPS reduction (pruned): {(baseline_flops - pruned_flops)/baseline_flops:.2%}\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    print(f\"Pruned:     Acc/metric={pruned_acc} | FLOPS={pruned_flops} | MFLOPS={pruned_mflops:.6f} | GFLOPS={pruned_gflops:.10f} | Time={pruned_time_ms:.4f} ms/sample\")\n",
    "\n",
    "    # ---- NEW METRIC REDUCTIONS ADDED BELOW ----\n",
    "    if baseline_flops > 0:\n",
    "        flop_reduction = (baseline_flops - pruned_flops) / baseline_flops\n",
    "        gflop_reduction = (baseline_gflops - pruned_gflops) / baseline_gflops if baseline_gflops > 0 else 0\n",
    "    else:\n",
    "        flop_reduction = 0\n",
    "        gflop_reduction = 0\n",
    "\n",
    "    acc_reduction = baseline_acc - pruned_acc if baseline_acc is not None else None\n",
    "\n",
    "    print(f\"FLOPS reduction (pruned): {flop_reduction:.2%}\")\n",
    "    print(f\"GFLOPS reduction (pruned): {gflop_reduction:.2%}\")\n",
    "    print(f\"Accuracy reduction: {acc_reduction}\")\n",
    "    # ---- END NEW LINES ----\n",
    "\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    # save masked/pruned models next to original\n",
    "    base = os.path.basename(model_path)\n",
    "    root = os.path.splitext(base)[0]\n",
    "    folder = os.path.dirname(model_path) or \".\"\n",
    "\n",
    "    masked_path = os.path.join(folder, root + \"_masked.h5\")\n",
    "    pruned_path = os.path.join(folder, root + \"_pruned.h5\")\n",
    "\n",
    "    try:\n",
    "        print(f\"[INFO] Saving masked model to: {masked_path}\")\n",
    "        masked_model.save(masked_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to save masked model: {e}\")\n",
    "\n",
    "    try:\n",
    "        print(f\"[INFO] Saving pruned model to: {pruned_path}\")\n",
    "        pruned_model.save(pruned_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to save pruned model: {e}\")\n",
    "\n",
    "    # return X_val for further external checks if desired\n",
    "    return model, masked_model, pruned_model, masks, X_val\n",
    "\n",
    "# =====================================================================\n",
    "# 9) RUN (example)\n",
    "# =====================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # change these to your files\n",
    "    model_path = r\"D:\\college\\sem-8\\models\\dementia_fnn_model.h5\"\n",
    "    dataset_path = r\"D:\\college\\sem-8\\dataset\\clean_dementia_dataset.csv\"\n",
    "\n",
    "    original_model, masked_model, pruned_model, masks, X_val = fnn_pruning_pipeline(\n",
    "        model_path=model_path,\n",
    "        dataset_path=dataset_path,\n",
    "        keep_ratio=0.8,\n",
    "        alpha=0.5, beta=0.3, gamma=0.2,\n",
    "        calib_batches=30, batch_size=64, ft_epochs=15\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76786061",
   "metadata": {},
   "source": [
    "# **Main CNN Pruned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3edddc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sequential_cnn_pruning_full_fixed.py\n",
    "Full Sequential CNN pruning pipeline (single-file)\n",
    "\n",
    "- Loads user Sequential CNN (.h5/.keras / SavedModel)\n",
    "- Sanitizes layer names in .h5 if they contain '/'\n",
    "- Loads folder dataset (image_dataset_from_directory)\n",
    "- Computes activation/gradient/variance importance\n",
    "- Creates masks (keep_ratio)\n",
    "- Builds masked model (gating layer)\n",
    "- Fine-tunes masked model\n",
    "- Structurally prunes model (conv filter removal, prune Dense outputs only)\n",
    "- Computes FLOPS & timings, evaluates models\n",
    "- Saves models and masks\n",
    "\n",
    "Fixes:\n",
    "- Ensures final layer matches dataset classes (automatically rebuilds final layer if needed)\n",
    "- Uses appropriate loss function (binary vs sparse categorical) during stat collection and training\n",
    "- Adds GFLOPS, accuracy reduction, inference timing\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ---------------------------\n",
    "# USER CONFIG (edit paths)\n",
    "# ---------------------------\n",
    "MODEL_PATH = r\"D:\\college\\sem-8\\models\\garbage_cnn_model.h5\"   # path to model (.h5/.keras or SavedModel dir)\n",
    "DATASET_PATH = r\"D:\\college\\sem-8\\dataset\\Garbage classification\\Garbage classification\"      # folder with subfolders per class\n",
    "SAVE_DIR = r\"pruning_output\"                                 # where outputs are saved\n",
    "base_name = os.path.splitext(os.path.basename(MODEL_PATH))[0]\n",
    "\n",
    "\n",
    "KEEP_RATIO = 0.83            # fraction of channels/units to keep\n",
    "ALPHA, BETA, GAMMA = 0.4, 0.3, 0.3  # importance weights\n",
    "BATCH_SIZE = 64\n",
    "CALIB_BATCHES = 30\n",
    "FT_EPOCHS = 3\n",
    "FT_BATCHES_TO_USE = 150\n",
    "PLOT_RESULTS = True\n",
    "VERBOSE = True\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def log(*args):\n",
    "    if VERBOSE:\n",
    "        print(\"[INFO]\", *args)\n",
    "\n",
    "# ---------------------------\n",
    "# Safe load model (handles '/' in h5 layer names)\n",
    "# ---------------------------\n",
    "def safe_load_model(model_path):\n",
    "    \"\"\"\n",
    "    Try normal load_model; if fails (e.g., '/' in layer names in H5), sanitize model_config JSON and rebuild.\n",
    "    Returns a Keras model (compiled=False).\n",
    "    \"\"\"\n",
    "    log(\"Loading model:\", model_path)\n",
    "    # Try direct load first\n",
    "    try:\n",
    "        m = tf.keras.models.load_model(model_path, compile=False)\n",
    "        log(\"Loaded model directly.\")\n",
    "        return m\n",
    "    except Exception as e:\n",
    "        log(\"Direct load failed:\", e)\n",
    "\n",
    "    # If HDF5, attempt to sanitize layer names in model_config\n",
    "    try:\n",
    "        with h5py.File(model_path, \"r\") as f:\n",
    "            if \"model_config\" in f:\n",
    "                raw = f[\"model_config\"][()]\n",
    "                if isinstance(raw, bytes):\n",
    "                    raw = raw.decode(\"utf-8\")\n",
    "                cfg_json = json.loads(raw)\n",
    "                changed = False\n",
    "                for layer in cfg_json.get(\"config\", {}).get(\"layers\", []):\n",
    "                    cfg = layer.get(\"config\", {})\n",
    "                    name = cfg.get(\"name\")\n",
    "                    if isinstance(name, str) and \"/\" in name:\n",
    "                        new_name = name.replace(\"/\", \"_\")\n",
    "                        cfg[\"name\"] = new_name\n",
    "                        changed = True\n",
    "                        log(f\"[FIX] layer name: {name} -> {new_name}\")\n",
    "                if changed:\n",
    "                    fixed_json = json.dumps(cfg_json)\n",
    "                    model = tf.keras.models.model_from_json(fixed_json)\n",
    "                    model.load_weights(model_path)\n",
    "                    log(\"Loaded model from sanitized JSON + weights.\")\n",
    "                    return model\n",
    "    except Exception as e2:\n",
    "        log(\"H5 sanitization attempt failed:\", e2)\n",
    "\n",
    "    # final fallback: try load_model with safe_mode=False (older TF)\n",
    "    try:\n",
    "        m = tf.keras.models.load_model(model_path, compile=False, safe_mode=False)\n",
    "        log(\"Loaded model with safe_mode=False.\")\n",
    "        return m\n",
    "    except Exception as e3:\n",
    "        log(\"All load attempts failed:\", e3)\n",
    "        raise RuntimeError(\"Failed to load model. Ensure path and format are correct.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Rebuild final layer to match num_classes (safe, best-effort)\n",
    "# ---------------------------\n",
    "def ensure_output_matches_dataset(orig_model, num_classes):\n",
    "    \"\"\"\n",
    "    If the model's current output shape doesn't match num_classes, rebuild final output\n",
    "    to match. Works for Sequential-style networks. Returns new_model, loss_fn.\n",
    "    \"\"\"\n",
    "    # Determine model's output dim\n",
    "    out_shape = tuple(orig_model.output_shape) if orig_model.output_shape is not None else None\n",
    "    # If already matches (and for multiclass softmax case), return original and appropriate loss.\n",
    "    if out_shape is not None:\n",
    "        if num_classes == 2 and (out_shape[-1] == 1 or out_shape[-1] == 2):\n",
    "            # binary case: allow Dense(1) or Dense(2) (Dense(2) could be softmax but labels are 0/1)\n",
    "            log(\"Model output seems compatible with binary classification.\")\n",
    "            loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False) if out_shape[-1] == 1 else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "            return orig_model, loss_fn\n",
    "        if num_classes > 2 and out_shape[-1] == num_classes:\n",
    "            log(\"Model output matches dataset classes.\")\n",
    "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "            return orig_model, loss_fn\n",
    "\n",
    "    # Need to rebuild final layer\n",
    "    log(\"Rebuilding model final layer to match num_classes:\", num_classes)\n",
    "    # We'll clone all layers except the last one and then append a new final dense.\n",
    "    # Use layer.from_config to avoid reusing layer objects in two models.\n",
    "    new_seq = models.Sequential(name=(orig_model.name or \"rebuilt_model\") + \"_rebuilt\")\n",
    "    # Add InputLayer\n",
    "    input_shape = orig_model.input_shape[1:]\n",
    "    new_seq.add(layers.InputLayer(input_shape=tuple(input_shape)))\n",
    "\n",
    "    # Clone all layers except the last (we will replace last)\n",
    "    # We'll attempt to copy weights for layers that remain identical\n",
    "    for layer in orig_model.layers[:-1]:\n",
    "        try:\n",
    "            cfg = layer.get_config()\n",
    "            Cls = layer.__class__\n",
    "            cloned = Cls.from_config(cfg)\n",
    "            new_seq.add(cloned)\n",
    "            # set weights if possible and shapes match\n",
    "            try:\n",
    "                w = layer.get_weights()\n",
    "                if w:\n",
    "                    new_seq.layers[-1].set_weights(w)\n",
    "            except Exception:\n",
    "                # ignore weight copy failures\n",
    "                pass\n",
    "        except Exception:\n",
    "            # fallback: try to append original layer (may cause errors but best-effort)\n",
    "            try:\n",
    "                new_seq.add(layer)\n",
    "            except Exception:\n",
    "                log(\"Warning: couldn't clone layer\", layer.name, \"- skipping weights copy.\")\n",
    "\n",
    "    # Add new final layer depending on num_classes\n",
    "    if num_classes == 2:\n",
    "        # binary: Dense(1, activation='sigmoid')\n",
    "        new_seq.add(layers.Dense(1, activation=\"sigmoid\", name=\"output_rebuilt\"))\n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    else:\n",
    "        new_seq.add(layers.Dense(num_classes, activation=\"softmax\", name=\"output_rebuilt\"))\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    # Try to compile minimal to ensure shape correctness (we'll let caller compile fully later)\n",
    "    return new_seq, loss_fn\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset loader (folder-based)\n",
    "# ---------------------------\n",
    "def load_image_folder_dataset(path, image_size, batch_size=BATCH_SIZE):\n",
    "    log(\"Loading dataset folder:\", path, \"image_size:\", image_size)\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=42,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=42,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    rescaler = layers.Rescaling(1.0 / 255)\n",
    "    train_ds = train_ds.map(lambda x, y: (rescaler(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(lambda x, y: (rescaler(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "# ---------------------------\n",
    "# Activation & gradient stats (uses provided loss function)\n",
    "# ---------------------------\n",
    "def compute_activation_grad_stats(model, layer_names, dataset, loss_fn, max_batches=CALIB_BATCHES):\n",
    "    \"\"\"\n",
    "    For each layer in layer_names, compute:\n",
    "      - A: mean(abs(activation)) per filter/unit\n",
    "      - G: mean(abs(grad wrt activation)) per filter/unit\n",
    "      - V: variance(activation) per filter/unit\n",
    "    Returns: dict name -> (A, G, V)\n",
    "    \"\"\"\n",
    "    log(\"Computing activation & gradient stats...\")\n",
    "    results = {n: [] for n in layer_names}\n",
    "    grad_results = {n: [] for n in layer_names}\n",
    "    var_results = {n: [] for n in layer_names}\n",
    "\n",
    "    batch_count = 0\n",
    "    for x_batch, y_batch in dataset:\n",
    "        if batch_count >= max_batches:\n",
    "            break\n",
    "        batch_count += 1\n",
    "        layer_acts = {}\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            a = x_batch\n",
    "            for layer in model.layers:\n",
    "                a = layer(a)\n",
    "                if layer.name in layer_names:\n",
    "                    tape.watch(a)\n",
    "                    layer_acts[layer.name] = a\n",
    "            preds = a\n",
    "            # compute loss using provided loss_fn (works for both binary & sparse categorical)\n",
    "            # When loss_fn is a Keras loss instance, call like a function\n",
    "            # Ensure y_batch dtype is int for sparse CE, float for binary BCE\n",
    "            try:\n",
    "                loss_vals = loss_fn(y_batch, preds)\n",
    "            except Exception:\n",
    "                # fallback attempt: convert types for binary\n",
    "                if isinstance(loss_fn, tf.keras.losses.BinaryCrossentropy):\n",
    "                    loss_vals = loss_fn(tf.cast(y_batch, tf.float32), preds)\n",
    "                else:\n",
    "                    loss_vals = loss_fn(y_batch, preds)\n",
    "            loss = tf.reduce_mean(loss_vals)\n",
    "\n",
    "        for name in layer_names:\n",
    "            a = layer_acts[name]\n",
    "            if len(a.shape) == 4:\n",
    "                A = tf.reduce_mean(tf.abs(a), axis=(0,1,2)).numpy()\n",
    "                V = tf.math.reduce_variance(a, axis=(0,1,2)).numpy()\n",
    "            else:\n",
    "                A = tf.reduce_mean(tf.abs(a), axis=0).numpy()\n",
    "                V = tf.math.reduce_variance(a, axis=0).numpy()\n",
    "\n",
    "            grad = tape.gradient(loss, a)\n",
    "            if grad is None:\n",
    "                G = np.zeros_like(A)\n",
    "            else:\n",
    "                if len(grad.shape) == 4:\n",
    "                    G = tf.reduce_mean(tf.abs(grad), axis=(0,1,2)).numpy()\n",
    "                else:\n",
    "                    G = tf.reduce_mean(tf.abs(grad), axis=0).numpy()\n",
    "\n",
    "            results[name].append(A)\n",
    "            var_results[name].append(V)\n",
    "            grad_results[name].append(G)\n",
    "        del tape\n",
    "\n",
    "    stats = {}\n",
    "    for name in layer_names:\n",
    "        A = np.mean(results[name], axis=0)\n",
    "        V = np.mean(var_results[name], axis=0)\n",
    "        G = np.mean(grad_results[name], axis=0)\n",
    "        stats[name] = (A, G, V)\n",
    "        log(f\"{name}: len={len(A)} meanA={A.mean():.6e} meanG={G.mean():.6e} meanV={V.mean():.6e}\")\n",
    "    return stats\n",
    "\n",
    "# ---------------------------\n",
    "# Importance scores & masks\n",
    "# ---------------------------\n",
    "def compute_importance_scores(stats, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n",
    "    def normalize(x):\n",
    "        x = x - x.min()\n",
    "        if x.max() > 0:\n",
    "            x = x / x.max()\n",
    "        return x\n",
    "    scores = {}\n",
    "    for name, (A, G, V) in stats.items():\n",
    "        scores[name] = alpha * normalize(A) + beta * normalize(G) + gamma * normalize(V)\n",
    "    return scores\n",
    "\n",
    "def make_masks_from_scores(score_map, keep_ratio=KEEP_RATIO):\n",
    "    masks = {}\n",
    "    for name, score in score_map.items():\n",
    "        k = max(1, int(len(score) * keep_ratio))\n",
    "        thresh = np.partition(score, -k)[-k]\n",
    "        mask = (score >= thresh).astype(np.float32)\n",
    "        masks[name] = mask\n",
    "        log(f\"{name}: keep {int(mask.sum())}/{len(mask)}\")\n",
    "    return masks\n",
    "\n",
    "# ---------------------------\n",
    "# Mask gate layer & masked model\n",
    "# ---------------------------\n",
    "class CNNGate(tf.keras.layers.Layer):\n",
    "    def __init__(self, channels, mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        channels: int number of channels this gate controls\n",
    "        mask: 1D array-like of length 'channels' with 0/1 values (or floats in [0,1]).\n",
    "              If provided, gate variable is initialized to these values; otherwise ones.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.channels = int(channels)\n",
    "        # store mask as numpy array if provided (for serialization convenience)\n",
    "        self._init_mask = None if mask is None else np.array(mask, dtype=np.float32)\n",
    "        init_val = self._init_mask if self._init_mask is not None else np.ones((self.channels,), dtype=np.float32)\n",
    "        # gate is non-trainable scalar per-channel multiplier\n",
    "        self.gate = self.add_weight(\n",
    "            name=\"gate\",\n",
    "            shape=(self.channels,),\n",
    "            initializer=tf.keras.initializers.Constant(init_val),\n",
    "            trainable=False,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # supports inputs with shape [batch, H, W, C] or [batch, C] (works broadcasting)\n",
    "        g = self.gate\n",
    "        # expand dims to match channels in conv output\n",
    "        if len(inputs.shape) == 4:\n",
    "            return inputs * g[None, None, None, :]\n",
    "        elif len(inputs.shape) == 2:\n",
    "            return inputs * g[None, :]\n",
    "        else:\n",
    "            # fallback broadcasting\n",
    "            return inputs * g\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        # do NOT embed gate values directly here (weights are saved by Keras),\n",
    "        # but we include channels for reconstruction convenience.\n",
    "        cfg.update({\n",
    "            \"channels\": self.channels,\n",
    "            # don't include mask/gate here to avoid duplicating weight data;\n",
    "            # the gate weight will be saved/loaded normally by Keras.\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # config may only contain \"channels\" â€” weight values will be restored by Keras load.\n",
    "        channels = config.pop(\"channels\")\n",
    "        return cls(channels=channels, **config)\n",
    "\n",
    "\n",
    "def build_masked_model(orig_model, masks):\n",
    "    \"\"\"\n",
    "    Build a new functional model with cloned layers from orig_model and insert a CNNGate\n",
    "    after each layer named in `masks`. The gate is initialized from masks[layer_name].\n",
    "    \"\"\"\n",
    "    log(\"Building masked model with gates (cloning layers)...\")\n",
    "    inp = tf.keras.Input(shape=orig_model.input_shape[1:])\n",
    "    x = inp\n",
    "\n",
    "    # keep mapping from original layer name -> new layer object (for weight copying)\n",
    "    for layer in orig_model.layers:\n",
    "        # clone layer if possible to avoid reusing original layer objects\n",
    "        try:\n",
    "            cfg = layer.get_config()\n",
    "            Cls = layer.__class__\n",
    "            new_layer = Cls.from_config(cfg)\n",
    "        except Exception:\n",
    "            # fallback: try to reuse the layer (less safe)\n",
    "            new_layer = layer\n",
    "\n",
    "        # apply the new layer to current tensor\n",
    "        x = new_layer(x)\n",
    "\n",
    "        # copy weights if layer had weights and we cloned it\n",
    "        try:\n",
    "            w = layer.get_weights()\n",
    "            if w:\n",
    "                try:\n",
    "                    new_layer.set_weights(w)\n",
    "                except Exception:\n",
    "                    # some layers (e.g., fused ops) may not accept direct set_weights â€” ignore\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # insert gate if this original layer is in masks\n",
    "        if layer.name in masks:\n",
    "            mask = np.array(masks[layer.name], dtype=np.float32)\n",
    "            channels = int(mask.shape[0])\n",
    "            gate_layer = CNNGate(channels=channels, mask=mask, name=layer.name + \"_gate\")\n",
    "            x = gate_layer(x)\n",
    "            # gate weight is already initialized from mask in CNNGate.__init__, so nothing else to do\n",
    "\n",
    "    masked = tf.keras.Model(inputs=inp, outputs=x, name=(orig_model.name or \"model\") + \"_masked\")\n",
    "    log(\"Masked model created:\", masked.name)\n",
    "    return masked\n",
    "\n",
    "# ---------------------------\n",
    "# Structural pruning (safe)\n",
    "# ---------------------------\n",
    "def prune_structural_sequential(orig_model, masks, input_shape):\n",
    "    \"\"\"\n",
    "    Structural pruning for Sequential models.\n",
    "    - Prune Conv2D output filters using masks[layer.name]\n",
    "    - Prune Dense output units only (do not slice Dense input rows that come from Flatten/Conv)\n",
    "    - Attempt to slice BatchNorm params to match conv outputs\n",
    "    \"\"\"\n",
    "    log(\"Structural pruning (safe) start...\")\n",
    "    new_layers = []\n",
    "    prev_was_conv_like = False  # indicates that Flatten/Conv preceded Dense inputs\n",
    "\n",
    "    for layer in orig_model.layers:\n",
    "        # Conv2D: slice output channels\n",
    "        if isinstance(layer, layers.Conv2D):\n",
    "            W, b = layer.get_weights()\n",
    "            orig_out = W.shape[-1]\n",
    "            mask = masks.get(layer.name, np.ones(orig_out, dtype=np.float32))\n",
    "            keep_idx = np.where(mask == 1)[0]\n",
    "            if keep_idx.size == 0:\n",
    "                keep_idx = np.array([int(np.argmax(mask))], dtype=np.int32)\n",
    "            W_new = W[:, :, :, keep_idx]\n",
    "            b_new = b[keep_idx]\n",
    "            new_conv = layers.Conv2D(\n",
    "                filters=len(keep_idx),\n",
    "                kernel_size=layer.kernel_size,\n",
    "                strides=layer.strides,\n",
    "                padding=layer.padding,\n",
    "                activation=layer.activation,\n",
    "                use_bias=layer.use_bias,\n",
    "                name=layer.name + \"_pruned\"\n",
    "            )\n",
    "            new_layers.append((new_conv, [W_new, b_new]))\n",
    "            prev_was_conv_like = True\n",
    "            continue\n",
    "\n",
    "        # BatchNorm: slice params if prev was conv-like\n",
    "        if isinstance(layer, layers.BatchNormalization):\n",
    "            try:\n",
    "                weights = layer.get_weights()\n",
    "                if prev_was_conv_like and new_layers:\n",
    "                    prev_layer_obj, prev_w = new_layers[-1]\n",
    "                    if prev_w is not None:\n",
    "                        out_ch = prev_w[0].shape[-1]  # kernel last dim\n",
    "                        gamma, beta, mean, var = weights\n",
    "                        gamma = gamma[:out_ch]\n",
    "                        beta = beta[:out_ch]\n",
    "                        mean = mean[:out_ch]\n",
    "                        var = var[:out_ch]\n",
    "                        new_bn = layers.BatchNormalization.from_config(layer.get_config())\n",
    "                        new_layers.append((new_bn, [gamma, beta, mean, var]))\n",
    "                        continue\n",
    "            except Exception:\n",
    "                pass\n",
    "            # fallback keep BN as-is\n",
    "            try:\n",
    "                new_bn = layers.BatchNormalization.from_config(layer.get_config())\n",
    "                new_layers.append((new_bn, layer.get_weights()))\n",
    "            except Exception:\n",
    "                new_layers.append((layer, layer.get_weights() if hasattr(layer, \"get_weights\") else None))\n",
    "            # prev_was_conv_like unchanged\n",
    "            continue\n",
    "\n",
    "        # MaxPool/Activation/Dropout/Flatten/GlobalAvgPool: clone or reuse\n",
    "        if isinstance(layer, (layers.MaxPooling2D, layers.Activation, layers.ReLU, layers.Dropout, layers.Flatten, layers.GlobalAveragePooling2D)):\n",
    "            try:\n",
    "                cloned = layer.__class__.from_config(layer.get_config())\n",
    "                w = layer.get_weights() if hasattr(layer, \"get_weights\") else None\n",
    "                new_layers.append((cloned, w if w else None))\n",
    "            except Exception:\n",
    "                new_layers.append((layer, layer.get_weights() if hasattr(layer, \"get_weights\") else None))\n",
    "            if isinstance(layer, layers.Flatten) or isinstance(layer, layers.GlobalAveragePooling2D):\n",
    "                prev_was_conv_like = True\n",
    "            continue\n",
    "\n",
    "        # Dense: prune outputs only (safe)\n",
    "        if isinstance(layer, layers.Dense):\n",
    "            W, b = layer.get_weights()  # shape (in_dim, out_dim)\n",
    "            out_mask = masks.get(layer.name, np.ones(W.shape[1], dtype=np.float32))\n",
    "            out_idx = np.where(out_mask == 1)[0]\n",
    "            if out_idx.size == 0:\n",
    "                out_idx = np.array([int(np.argmax(out_mask))], dtype=np.int32)\n",
    "            W_new = W[:, out_idx]   # keep all input rows (safe)\n",
    "            b_new = b[out_idx]\n",
    "            new_dense = layers.Dense(units=W_new.shape[1], activation=layer.activation, name=layer.name + \"_pruned\")\n",
    "            new_layers.append((new_dense, [W_new, b_new]))\n",
    "            prev_was_conv_like = False\n",
    "            continue\n",
    "\n",
    "        # Fallback for other layers\n",
    "        try:\n",
    "            cloned = layer.__class__.from_config(layer.get_config())\n",
    "            w = layer.get_weights() if hasattr(layer, \"get_weights\") else None\n",
    "            new_layers.append((cloned, w if w else None))\n",
    "        except Exception:\n",
    "            new_layers.append((layer, layer.get_weights() if hasattr(layer, \"get_weights\") else None))\n",
    "        prev_was_conv_like = False\n",
    "\n",
    "    # Build new Sequential model with InputLayer\n",
    "    seq = models.Sequential(name=orig_model.name + \"_struct_pruned\")\n",
    "    seq.add(layers.InputLayer(input_shape=tuple(input_shape)))\n",
    "    for lyr_obj, w in new_layers:\n",
    "        seq.add(lyr_obj)\n",
    "        if w is not None:\n",
    "            try:\n",
    "                seq.layers[-1].set_weights(w)\n",
    "            except Exception as e:\n",
    "                log(\"Warning: couldn't set weights for\", seq.layers[-1].name, \":\", e)\n",
    "    log(\"Structural pruning complete. New model summary:\")\n",
    "    seq.summary()\n",
    "    return seq\n",
    "\n",
    "# ---------------------------\n",
    "# FLOPS and timing helpers\n",
    "# ---------------------------\n",
    "def calculate_conv_flops(input_shape, kernel_shape, strides=(1,1), padding='same'):\n",
    "    h_in, w_in, c_in = input_shape\n",
    "    kh, kw, _, c_out = kernel_shape\n",
    "    if padding == 'same':\n",
    "        h_out = math.ceil(h_in / strides[0])\n",
    "        w_out = math.ceil(w_in / strides[1])\n",
    "    else:\n",
    "        h_out = math.ceil((h_in - kh + 1) / strides[0])\n",
    "        w_out = math.ceil((w_in - kw + 1) / strides[1])\n",
    "    flops = h_out * w_out * (kh * kw * c_in) * c_out * 2\n",
    "    return flops, (h_out, w_out, c_out)\n",
    "\n",
    "def calculate_dense_flops(in_size, out_size):\n",
    "    return in_size * out_size * 2\n",
    "\n",
    "def calculate_model_flops(model, input_shape):\n",
    "    total = 0\n",
    "    current_shape = tuple(input_shape)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, layers.Conv2D):\n",
    "            weights = layer.get_weights()\n",
    "            if not weights:\n",
    "                continue\n",
    "            kernel_shape = weights[0].shape  # (kh, kw, in_c, out_c)\n",
    "            layer_flops, current_shape = calculate_conv_flops(current_shape, kernel_shape, strides=layer.strides, padding=layer.padding)\n",
    "            total += layer_flops\n",
    "        elif isinstance(layer, layers.Flatten):\n",
    "            current_shape = (int(np.prod(current_shape)),)\n",
    "        elif isinstance(layer, layers.Dense):\n",
    "            in_size = current_shape[0] if isinstance(current_shape, tuple) and len(current_shape)>0 else int(current_shape)\n",
    "            layer_flops = calculate_dense_flops(in_size, layer.units)\n",
    "            total += layer_flops\n",
    "            current_shape = (layer.units,)\n",
    "        elif isinstance(layer, layers.MaxPooling2D):\n",
    "            h,w,c = current_shape\n",
    "            pool = layer.pool_size[0] if hasattr(layer.pool_size, \"__getitem__\") else layer.pool_size\n",
    "            current_shape = (h//pool, w//pool, c)\n",
    "        else:\n",
    "            # ignore other layers for shape changes\n",
    "            pass\n",
    "    return total\n",
    "\n",
    "def measure_inference_time(model, sample_batch, steps=20):\n",
    "    model.predict(sample_batch, verbose=0)  # warmup\n",
    "    t0 = time.time()\n",
    "    for _ in range(steps):\n",
    "        model.predict(sample_batch, verbose=0)\n",
    "    t1 = time.time()\n",
    "    return (t1 - t0) / steps\n",
    "\n",
    "# ---------------------------\n",
    "# Save masks helper\n",
    "# ---------------------------\n",
    "def save_masks(masks, path):\n",
    "    serial = {k: v.tolist() for k,v in masks.items()}\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(serial, f, indent=2)\n",
    "    log(\"Saved masks to\", path)\n",
    "\n",
    "# ---------------------------\n",
    "# Plot helper\n",
    "# ---------------------------\n",
    "def plot_mask_histograms(masks, outdir=SAVE_DIR):\n",
    "    if not PLOT_RESULTS:\n",
    "        return\n",
    "    for name, mask in masks.items():\n",
    "        plt.figure(figsize=(5,2))\n",
    "        plt.title(name)\n",
    "        plt.hist(mask, bins=2)\n",
    "        plt.xlabel(\"0=pruned, 1=kept\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"mask_{name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN pipeline\n",
    "# ---------------------------\n",
    "def full_pipeline(model_path, dataset_path):\n",
    "    # 1) load model (safe)\n",
    "    model = safe_load_model(model_path)\n",
    "    log(\"Model loaded. Summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    # 2) load dataset and infer num_classes\n",
    "    # We need input size so infer from model\n",
    "    input_shape = model.input_shape[1:]\n",
    "    log(\"Inferred input shape:\", input_shape)\n",
    "\n",
    "    train_ds, val_ds = load_image_folder_dataset(dataset_path, image_size=input_shape[:2], batch_size=BATCH_SIZE)\n",
    "    # Determine number of classes from dataset\n",
    "    try:\n",
    "        # tf.data.Dataset from image_dataset_from_directory has .class_names on the original Dataset returned object,\n",
    "        # but not on the batched dataset â€” so inspect via a fresh loader:\n",
    "        tmp = tf.keras.utils.image_dataset_from_directory(dataset_path, image_size=input_shape[:2], batch_size=1)\n",
    "        num_classes = len(tmp.class_names)\n",
    "        del tmp\n",
    "    except Exception:\n",
    "        # fallback: infer from labels in train_ds\n",
    "        classes = set()\n",
    "        for _, y in train_ds.take(10):\n",
    "            classes.update(y.numpy().tolist())\n",
    "        num_classes = max(classes) + 1 if classes else 2\n",
    "\n",
    "    log(\"Detected dataset classes (num_classes):\", num_classes)\n",
    "\n",
    "    # 3) ensure model output matches dataset classes\n",
    "    model, loss_fn = ensure_output_matches_dataset(model, num_classes)\n",
    "    # compile original so evaluate works (use small lr default) with detected loss\n",
    "    if isinstance(loss_fn, tf.keras.losses.BinaryCrossentropy):\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_fn, metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_fn, metrics=[\"accuracy\"])\n",
    "\n",
    "    # reprint summary\n",
    "    log(\"Final model used (after potential rebuild). Summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    # calibration subset for stats\n",
    "    calib_ds = train_ds.take(CALIB_BATCHES)\n",
    "\n",
    "    # sample for timing\n",
    "    try:\n",
    "        sample_x, _ = next(iter(val_ds))\n",
    "    except Exception:\n",
    "        sample_x, _ = next(iter(train_ds))\n",
    "    sample_x_small = sample_x[:min(16, sample_x.shape[0])]\n",
    "\n",
    "    # 4) choose layers to prune (conv + hidden dense only, not final output)\n",
    "    dense_layers = [lyr for lyr in model.layers if isinstance(lyr, layers.Dense)]\n",
    "    last_dense = dense_layers[-1] if dense_layers else None\n",
    "\n",
    "    prune_layer_names = []\n",
    "    for lyr in model.layers:\n",
    "        if isinstance(lyr, layers.Conv2D):\n",
    "            prune_layer_names.append(lyr.name)\n",
    "        elif isinstance(lyr, layers.Dense) and lyr is not last_dense:\n",
    "            prune_layer_names.append(lyr.name)\n",
    "    log(\"Layers considered for pruning:\", prune_layer_names)\n",
    "\n",
    "    # 5) compute stats (pass loss_fn)\n",
    "    stats = compute_activation_grad_stats(model, prune_layer_names, calib_ds, loss_fn=loss_fn, max_batches=CALIB_BATCHES)\n",
    "\n",
    "    # 6) importance & masks\n",
    "    score_map = compute_importance_scores(stats)\n",
    "    masks = make_masks_from_scores(score_map, keep_ratio=KEEP_RATIO)\n",
    "    save_masks(masks, os.path.join(SAVE_DIR, \"masks.json\"))\n",
    "    plot_mask_histograms(masks)\n",
    "\n",
    "    # 7) build masked model and compile (use same loss)\n",
    "    masked_model = build_masked_model(model, masks)\n",
    "    masked_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=loss_fn, metrics=[\"accuracy\"])\n",
    "    log(\"Masked model built.\")\n",
    "\n",
    "    # quick eval before FT (small subset)\n",
    "        # quick eval before FT (small subset)\n",
    "    try:\n",
    "        loss0, acc0 = masked_model.evaluate(val_ds.take(5), verbose=0)\n",
    "        log(\"Masked model pre-FT acc:\", acc0)\n",
    "    except Exception as e:\n",
    "        log(\"Masked model pre-eval failed:\", e)\n",
    "        acc0 = None\n",
    "\n",
    "    # 8) measure baseline flops & time\n",
    "    baseline_flops = calculate_model_flops(model, input_shape)\n",
    "    baseline_time = measure_inference_time(model, sample_x_small, steps=10)\n",
    "    log(f\"Baseline FLOPS: {baseline_flops:,}, baseline time (avg batch): {baseline_time:.4f}s\")\n",
    "\n",
    "    # 9) fine-tune masked model\n",
    "    try:\n",
    "        masked_model.fit(\n",
    "            train_ds.take(FT_BATCHES_TO_USE),\n",
    "            validation_data=val_ds.take(5),\n",
    "            epochs=FT_EPOCHS,\n",
    "            verbose=2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log(\"Masked fine-tune failed/partial:\", e)\n",
    "\n",
    "    # 10) structural prune\n",
    "    # NOTE: you can prune the original model or the masked_model.\n",
    "    #       pruning masked_model preserves gating decisions â€” often desired.\n",
    "    try:\n",
    "        pruned_model = prune_structural_sequential(masked_model, masks, input_shape)\n",
    "    except Exception:\n",
    "        # fallback to pruning original model if pruning masked_model fails\n",
    "        pruned_model = prune_structural_sequential(model, masks, input_shape)\n",
    "\n",
    "    # compile pruned model for evaluation / training with same loss\n",
    "    pruned_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=loss_fn,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    log(\"Starting fine-tuning of PRUNED model (EarlyStopping)...\")\n",
    "    early_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=2,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    try:\n",
    "        pruned_model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=20,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[early_cb],\n",
    "            verbose=2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log(\"Pruned model fine-tune failed/partial:\", e)\n",
    "\n",
    "    # 11) evaluate models\n",
    "    log(\"Evaluating Original model:\")\n",
    "    orig_loss, orig_acc = model.evaluate(val_ds, verbose=0)\n",
    "    log(\"Original acc:\", orig_acc)\n",
    "\n",
    "    log(\"Evaluating Masked model (after FT):\")\n",
    "    try:\n",
    "        mask_loss, mask_acc = masked_model.evaluate(val_ds, verbose=0)\n",
    "        log(\"Masked acc:\", mask_acc)\n",
    "    except Exception as e:\n",
    "        log(\"Masked evaluate failed:\", e)\n",
    "        mask_acc = None\n",
    "\n",
    "    log(\"Evaluating Pruned model:\")\n",
    "    try:\n",
    "        pruned_loss, pruned_acc = pruned_model.evaluate(val_ds, verbose=0)\n",
    "        log(\"Pruned acc:\", pruned_acc)\n",
    "    except Exception as e:\n",
    "        log(\"Pruned evaluate failed:\", e)\n",
    "        pruned_acc = None\n",
    "\n",
    "    # 12) FLOPS & timing after prune\n",
    "    pruned_flops = calculate_model_flops(pruned_model, input_shape)\n",
    "    pruned_time = measure_inference_time(pruned_model, sample_x_small, steps=10)\n",
    "    log(f\"Pruned FLOPS: {pruned_flops:,}, pruned time: {pruned_time:.4f}s\")\n",
    "\n",
    "    # 13) summary & save\n",
    "    reduction = 1.0 - (pruned_flops / baseline_flops) if baseline_flops > 0 else 0.0\n",
    "    log(\"=\"*60)\n",
    "    log(\"SUMMARY:\")\n",
    "    log(f\"Baseline FLOPS: {baseline_flops:,}\")\n",
    "    log(f\"Pruned FLOPS: {pruned_flops:,}\")\n",
    "    log(f\"FLOPS reduction: {reduction:.2%}\")\n",
    "    log(f\"Original acc: {orig_acc}, Masked acc: {mask_acc}, Pruned acc: {pruned_acc}\")\n",
    "    log(\"=\"*60)\n",
    "\n",
    "    # ---- Extra Metrics: GFLOPS + Accuracy Reduction ----\n",
    "    baseline_gflops = baseline_flops / 1e9\n",
    "    pruned_gflops = pruned_flops / 1e9\n",
    "    gflops_reduction = 1.0 - (pruned_gflops / baseline_gflops) if baseline_gflops > 0 else 0.0\n",
    "    acc_reduction = (orig_acc - pruned_acc) if (orig_acc is not None and pruned_acc is not None) else None\n",
    "\n",
    "    log(f\"Baseline GFLOPS: {baseline_gflops:.4f}\")\n",
    "    log(f\"Pruned GFLOPS: {pruned_gflops:.4f}\")\n",
    "    log(f\"GFLOPS reduction: {gflops_reduction:.2%}\")\n",
    "    if acc_reduction is not None:\n",
    "        log(f\"Accuracy reduction: {acc_reduction:.4f}\")\n",
    "    else:\n",
    "        log(\"Accuracy reduction: N/A\")\n",
    "\n",
    "    # save artifacts\n",
    "    try:\n",
    "        # use proper file extensions to avoid Keras save errors\n",
    "        baseline_name = base_name + \"_baseline.keras\"\n",
    "        masked_name   = base_name + \"_masked.keras\"\n",
    "        pruned_name   = base_name + \"_pruned.keras\"\n",
    "\n",
    "        model.save(os.path.join(SAVE_DIR, baseline_name))\n",
    "        masked_model.save(os.path.join(SAVE_DIR, masked_name))\n",
    "        pruned_model.save(os.path.join(SAVE_DIR, pruned_name))\n",
    "\n",
    "        log(f\"Saved models under names: {baseline_name}, {masked_name}, {pruned_name}\")\n",
    "\n",
    "        log(\"Saved models to\", SAVE_DIR)\n",
    "    except Exception as e:\n",
    "        log(\"Save models failed:\", e)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"masked_model\": masked_model,\n",
    "        \"pruned_model\": pruned_model,\n",
    "        \"masks\": masks,\n",
    "        \"baseline_flops\": baseline_flops,\n",
    "        \"pruned_flops\": pruned_flops,\n",
    "        \"baseline_time\": baseline_time,\n",
    "        \"pruned_time\": pruned_time,\n",
    "        \"orig_acc\": orig_acc,\n",
    "        \"mask_acc\": mask_acc,\n",
    "        \"pruned_acc\": pruned_acc\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Run\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    out = full_pipeline(MODEL_PATH, DATASET_PATH)\n",
    "    log(\"Pipeline finished. Outputs:\", out.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0614c8",
   "metadata": {},
   "source": [
    "# **Main Resnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50193024",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# RESNET50 SOFT CHANNEL MASKING (BN-SAFE, GRAPH-SAFE, SERIALIZABLE)\n",
    "# ==========================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MODEL_PATH   = \"/content/ishan_100__resnet.h5\"\n",
    "DATASET_PATH = \"/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train\"\n",
    "\n",
    "BATCH = 32\n",
    "KEEP_RATIO = 0.85\n",
    "ALPHA = 0.1\n",
    "EPOCHS = 10\n",
    "PATIENCE = 3\n",
    "UNFREEZE_LAST = 15\n",
    "img_size=(128,128)\n",
    "# ---------------- DATA ----------------\n",
    "def load_ds(path, img_size):\n",
    "    train = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=42,\n",
    "        image_size=img_size,\n",
    "        batch_size=BATCH\n",
    "    )\n",
    "    val = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=42,\n",
    "        image_size=img_size,\n",
    "        batch_size=BATCH\n",
    "    )\n",
    "\n",
    "    scale = layers.Rescaling(1. / 255)\n",
    "    train = train.map(lambda x, y: (scale(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    val   = val.map(lambda x, y: (scale(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    return train, val\n",
    "\n",
    "# ---------------- FIND CONVS ----------------\n",
    "def find_convs(model):\n",
    "    convs = []\n",
    "\n",
    "    def walk(l):\n",
    "        if isinstance(l, layers.Conv2D):\n",
    "            convs.append(l)\n",
    "        if isinstance(l, tf.keras.Model):\n",
    "            for x in l.layers:\n",
    "                walk(x)\n",
    "\n",
    "    walk(model)\n",
    "    return convs\n",
    "\n",
    "# ---------------- SOFT CHANNEL MASK ----------------\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class ChannelMask(layers.Layer):\n",
    "    def __init__(self, mask, alpha=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        mask = np.asarray(mask, dtype=\"float32\")\n",
    "        mask = np.where(mask > 0, 1.0, alpha)\n",
    "        mask = mask / np.mean(mask)\n",
    "        self.mask_init = mask\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mask = self.add_weight(\n",
    "            name=\"mask\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=tf.constant_initializer(self.mask_init),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.mask[None, None, None, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"mask\": self.mask_init.tolist(),\n",
    "            \"alpha\": self.alpha\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        mask = config.pop(\"mask\")\n",
    "        return cls(mask=mask, **config)\n",
    "\n",
    "# ---------------- MASK CREATION ----------------\n",
    "def make_masks(convs):\n",
    "    masks = {}\n",
    "    for l in convs:\n",
    "        W = l.kernel.numpy()\n",
    "        scores = np.mean(np.abs(W), axis=(0, 1, 2))\n",
    "        k = max(1, int(len(scores) * KEEP_RATIO))\n",
    "        thr = np.partition(scores, -k)[-k]\n",
    "        mask = (scores >= thr).astype(\"float32\")\n",
    "        masks[l.name] = mask\n",
    "        print(f\"[PRUNE] {l.name}: keep {int(mask.sum())}/{len(mask)}\")\n",
    "    return masks\n",
    "\n",
    "# ---------------- GRAPH-SAFE MASK INSERT ----------------\n",
    "def build_masked_resnet(base_model, masks):\n",
    "    tensor_map = {}\n",
    "    last_tensor = None\n",
    "\n",
    "    for inp in base_model.inputs:\n",
    "        tensor_map[inp] = inp\n",
    "        last_tensor = inp\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "\n",
    "        if isinstance(layer, layers.InputLayer):\n",
    "            continue\n",
    "\n",
    "        inbound = layer.input\n",
    "        if isinstance(inbound, list):\n",
    "            mapped = [tensor_map.get(x, x) for x in inbound]\n",
    "        else:\n",
    "            mapped = tensor_map.get(inbound, inbound)\n",
    "\n",
    "        x = layer(mapped)\n",
    "\n",
    "        # Insert mask AFTER BatchNorm\n",
    "        if isinstance(layer, layers.BatchNormalization):\n",
    "            prev = layer.input._keras_history[0]\n",
    "            if isinstance(prev, layers.Conv2D) and prev.name in masks:\n",
    "                x = ChannelMask(\n",
    "                    masks[prev.name],\n",
    "                    alpha=ALPHA,\n",
    "                    name=prev.name + \"_mask\"\n",
    "                )(x)\n",
    "\n",
    "        tensor_map[layer.output] = x\n",
    "        last_tensor = x\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=base_model.inputs,\n",
    "        outputs=last_tensor,\n",
    "        name=base_model.name + \"_masked\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------- FLOPs ----------------\n",
    "def baseline_flops(model):\n",
    "    total = 0\n",
    "    for l in find_convs(model):\n",
    "        if not l._inbound_nodes:\n",
    "            continue\n",
    "        _, h, w, cout = l.output.shape\n",
    "        cin = l.input.shape[-1]\n",
    "        k = l.kernel_size[0]\n",
    "        total += h * w * cin * cout * k * k * 2\n",
    "    return int(total)\n",
    "\n",
    "def effective_flops_from_masks(model, masks):\n",
    "    total = 0\n",
    "    for l in find_convs(model):\n",
    "        if l.name in masks and l._inbound_nodes:\n",
    "            _, h, w, _ = l.output.shape\n",
    "            cin = l.input.shape[-1]\n",
    "            k = l.kernel_size[0]\n",
    "            active = int(np.sum(masks[l.name]))\n",
    "            total += h * w * cin * active * k * k * 2\n",
    "    return int(total)\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "def run():\n",
    "    print(\"[INFO] Loading base model...\")\n",
    "    base = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "\n",
    "    # ðŸ”§ force base graph build\n",
    "    dummy = tf.zeros((1,) + base.input_shape[1:])\n",
    "    _ = base(dummy)\n",
    "\n",
    "    train, val = load_ds(DATASET_PATH, base.input_shape[1:3])\n",
    "\n",
    "    convs = find_convs(base)\n",
    "    masks = make_masks(convs)\n",
    "\n",
    "    masked = build_masked_resnet(base, masks)\n",
    "\n",
    "    # ðŸ”§ force masked graph build\n",
    "    _ = masked(dummy)\n",
    "\n",
    "    # freeze\n",
    "    for l in masked.layers[:-UNFREEZE_LAST]:\n",
    "        l.trainable = False\n",
    "\n",
    "    masked.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    early = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "\n",
    "    masked.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[early],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    masked.save(\"masked_finetuned_resnet_leaf.keras\")\n",
    "\n",
    "    b = baseline_flops(base)\n",
    "    e = effective_flops_from_masks(base, masks)\n",
    "\n",
    "    print(\"\\n=========== EFFECTIVE FLOPs ANALYSIS ===========\")\n",
    "    print(f\"Baseline FLOPs : {b:,}\")\n",
    "    print(f\"Effective FLOPs: {e:,}\")\n",
    "    print(f\"Reduction (%)  : {(b - e) / b * 100:.2f}%\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664453b",
   "metadata": {},
   "source": [
    "# **Main EfficientNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1a061",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MODEL_PATH   = \"/content/efficientnet_fruit_f.keras\"\n",
    "DATASET_PATH = \"/kaggle/input/fruits/fruits-360_100x100/fruits-360/Training\"\n",
    "\n",
    "BATCH = 32\n",
    "KEEP_RATIO = 0.85\n",
    "ALPHA = 0.1\n",
    "EPOCHS = 10\n",
    "PATIENCE = 3\n",
    "UNFREEZE_LAST = 20\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "# ---------------- DATA ----------------\n",
    "def load_ds(path, img_size):\n",
    "    train = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=42,\n",
    "        image_size=img_size,\n",
    "        batch_size=BATCH\n",
    "    )\n",
    "    val = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=42,\n",
    "        image_size=img_size,\n",
    "        batch_size=BATCH\n",
    "    )\n",
    "\n",
    "    scale = layers.Rescaling(1. / 255)\n",
    "    train = train.map(lambda x, y: (scale(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    val   = val.map(lambda x, y: (scale(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    return train, val\n",
    "\n",
    "\n",
    "# ---------------- FIND PRUNABLE CONVS ----------------\n",
    "# EfficientNet rule:\n",
    "# âœ” mask Conv2D with kernel_size=(1,1)\n",
    "# âœ˜ skip DepthwiseConv2D\n",
    "def find_prunable_convs(model):\n",
    "    convs = []\n",
    "\n",
    "    def walk(l):\n",
    "        if isinstance(l, layers.Conv2D):\n",
    "            if l.kernel_size == (1, 1):\n",
    "                convs.append(l)\n",
    "        if isinstance(l, tf.keras.Model):\n",
    "            for x in l.layers:\n",
    "                walk(x)\n",
    "\n",
    "    walk(model)\n",
    "    return convs\n",
    "\n",
    "\n",
    "# ---------------- SOFT CHANNEL MASK ----------------\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class ChannelMask(layers.Layer):\n",
    "    def __init__(self, mask, alpha=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        mask = np.asarray(mask, dtype=\"float32\")\n",
    "        mask = np.where(mask > 0, 1.0, alpha)\n",
    "        mask = mask / np.mean(mask)   # keep activation scale stable\n",
    "        self.mask_init = mask\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mask = self.add_weight(\n",
    "            name=\"mask\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=tf.constant_initializer(self.mask_init),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.mask[None, None, None, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"mask\": self.mask_init.tolist(),\n",
    "            \"alpha\": self.alpha\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        mask = config.pop(\"mask\")\n",
    "        return cls(mask=mask, **config)\n",
    "\n",
    "\n",
    "# ---------------- MASK CREATION ----------------\n",
    "def make_masks(convs):\n",
    "    masks = {}\n",
    "    for l in convs:\n",
    "        W = l.kernel.numpy()\n",
    "        scores = np.mean(np.abs(W), axis=(0, 1, 2))  # per-output-channel\n",
    "        k = max(1, int(len(scores) * KEEP_RATIO))\n",
    "        thr = np.partition(scores, -k)[-k]\n",
    "        mask = (scores >= thr).astype(\"float32\")\n",
    "        masks[l.name] = mask\n",
    "        print(f\"[PRUNE] {l.name}: keep {int(mask.sum())}/{len(mask)}\")\n",
    "    return masks\n",
    "\n",
    "\n",
    "# ---------------- GRAPH-SAFE MASK INSERT ----------------\n",
    "def build_masked_efficientnet(base_model, masks):\n",
    "    tensor_map = {}\n",
    "    last_tensor = None\n",
    "\n",
    "    for inp in base_model.inputs:\n",
    "        tensor_map[inp] = inp\n",
    "        last_tensor = inp\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "\n",
    "        if isinstance(layer, layers.InputLayer):\n",
    "            continue\n",
    "\n",
    "        inbound = layer.input\n",
    "        if isinstance(inbound, list):\n",
    "            mapped = [tensor_map.get(x, x) for x in inbound]\n",
    "        else:\n",
    "            mapped = tensor_map.get(inbound, inbound)\n",
    "\n",
    "        x = layer(mapped)\n",
    "\n",
    "        # Insert mask AFTER BatchNorm if previous layer was Conv2D(1x1)\n",
    "        if isinstance(layer, layers.BatchNormalization):\n",
    "            prev = layer.input._keras_history[0]\n",
    "            if isinstance(prev, layers.Conv2D):\n",
    "                if prev.name in masks:\n",
    "                    x = ChannelMask(\n",
    "                        masks[prev.name],\n",
    "                        alpha=ALPHA,\n",
    "                        name=prev.name + \"_mask\"\n",
    "                    )(x)\n",
    "\n",
    "        tensor_map[layer.output] = x\n",
    "        last_tensor = x\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=base_model.inputs,\n",
    "        outputs=last_tensor,\n",
    "        name=base_model.name + \"_masked\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------- FLOPs ----------------\n",
    "# EfficientNet FLOPs: count ONLY pointwise convs\n",
    "def baseline_flops(model):\n",
    "    total = 0\n",
    "    for l in find_prunable_convs(model):\n",
    "        if not l._inbound_nodes:\n",
    "            continue\n",
    "        _, h, w, cout = l.output.shape\n",
    "        cin = l.input.shape[-1]\n",
    "        total += h * w * cin * cout * 2\n",
    "    return int(total)\n",
    "\n",
    "\n",
    "def effective_flops_from_masks(model, masks):\n",
    "    total = 0\n",
    "    for l in find_prunable_convs(model):\n",
    "        if l.name in masks and l._inbound_nodes:\n",
    "            _, h, w, _ = l.output.shape\n",
    "            cin = l.input.shape[-1]\n",
    "            active = int(np.sum(masks[l.name]))\n",
    "            total += h * w * cin * active * 2\n",
    "    return int(total)\n",
    "\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "def run():\n",
    "    print(\"[INFO] Loading base EfficientNet...\")\n",
    "    base = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "\n",
    "    # ðŸ”§ force graph build\n",
    "    dummy = tf.zeros((1,) + base.input_shape[1:])\n",
    "    _ = base(dummy)\n",
    "\n",
    "    train, val = load_ds(DATASET_PATH, base.input_shape[1:3])\n",
    "\n",
    "    convs = find_prunable_convs(base)\n",
    "    masks = make_masks(convs)\n",
    "\n",
    "    masked = build_masked_efficientnet(base, masks)\n",
    "\n",
    "    # ðŸ”§ force masked graph build\n",
    "    _ = masked(dummy)\n",
    "\n",
    "    # freeze most layers\n",
    "    for l in masked.layers[:-UNFREEZE_LAST]:\n",
    "        l.trainable = False\n",
    "\n",
    "    masked.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    early = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "\n",
    "    masked.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[early],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    masked.save(\"masked_finetuned_efficientnet_fruit_image_classification.keras\")\n",
    "\n",
    "    b = baseline_flops(base)\n",
    "    e = effective_flops_from_masks(base, masks)\n",
    "\n",
    "    print(\"\\n=========== EFFECTIVE FLOPs ANALYSIS ===========\")\n",
    "    print(f\"Baseline FLOPs : {b:,}\")\n",
    "    print(f\"Effective FLOPs: {e:,}\")\n",
    "    print(f\"Reduction (%)  : {(b - e) / b * 100:.2f}%\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef8730",
   "metadata": {},
   "source": [
    "# **Main MobileNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480861f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MODEL_PATH   = \"/content/mobilenet_f.keras\"\n",
    "DATASET_PATH = \"/root/.cache/kagglehub/datasets/puneet6060/intel-image-classification/versions/2/seg_train/seg_train\"\n",
    "\n",
    "\n",
    "BATCH = 32\n",
    "KEEP_RATIO = 0.85\n",
    "ALPHA = 0.1\n",
    "EPOCHS = 10\n",
    "PATIENCE = 3\n",
    "UNFREEZE_LAST = 20\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "# ---------------- DATA ----------------\n",
    "def load_ds(path, img_size):\n",
    "    train = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=42,\n",
    "        image_size=img_size,\n",
    "        batch_size=BATCH\n",
    "    )\n",
    "    val = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=42,\n",
    "        image_size=img_size,\n",
    "        batch_size=BATCH\n",
    "    )\n",
    "\n",
    "    scale = layers.Rescaling(1. / 255)\n",
    "    train = train.map(lambda x, y: (scale(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    val   = val.map(lambda x, y: (scale(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    return train, val\n",
    "\n",
    "\n",
    "# ---------------- FIND PRUNABLE CONVS ----------------\n",
    "# MobileNet rule:\n",
    "# âœ” Conv2D kernel=(1,1)  (pointwise)\n",
    "# âœ˜ DepthwiseConv2D\n",
    "def find_prunable_convs(model):\n",
    "    convs = []\n",
    "\n",
    "    def walk(l):\n",
    "        if isinstance(l, layers.Conv2D):\n",
    "            if l.kernel_size == (1, 1):\n",
    "                convs.append(l)\n",
    "        if isinstance(l, tf.keras.Model):\n",
    "            for x in l.layers:\n",
    "                walk(x)\n",
    "\n",
    "    walk(model)\n",
    "    return convs\n",
    "\n",
    "\n",
    "# ---------------- SOFT CHANNEL MASK ----------------\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class ChannelMask(layers.Layer):\n",
    "    def __init__(self, mask, alpha=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        mask = np.asarray(mask, dtype=\"float32\")\n",
    "        mask = np.where(mask > 0, 1.0, alpha)\n",
    "        mask = mask / np.mean(mask)  # activation scale stability\n",
    "        self.mask_init = mask\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mask = self.add_weight(\n",
    "            name=\"mask\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=tf.constant_initializer(self.mask_init),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.mask[None, None, None, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"mask\": self.mask_init.tolist(),\n",
    "            \"alpha\": self.alpha\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        mask = config.pop(\"mask\")\n",
    "        return cls(mask=mask, **config)\n",
    "\n",
    "\n",
    "# ---------------- MASK CREATION ----------------\n",
    "def make_masks(convs):\n",
    "    masks = {}\n",
    "    for l in convs:\n",
    "        W = l.kernel.numpy()\n",
    "        scores = np.mean(np.abs(W), axis=(0, 1, 2))\n",
    "        k = max(1, int(len(scores) * KEEP_RATIO))\n",
    "        thr = np.partition(scores, -k)[-k]\n",
    "        mask = (scores >= thr).astype(\"float32\")\n",
    "        masks[l.name] = mask\n",
    "        print(f\"[PRUNE] {l.name}: keep {int(mask.sum())}/{len(mask)}\")\n",
    "    return masks\n",
    "\n",
    "\n",
    "# ---------------- GRAPH-SAFE MASK INSERT ----------------\n",
    "def build_masked_mobilenet(base_model, masks):\n",
    "    tensor_map = {}\n",
    "\n",
    "    for inp in base_model.inputs:\n",
    "        tensor_map[inp] = inp\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "\n",
    "        if isinstance(layer, layers.InputLayer):\n",
    "            continue\n",
    "\n",
    "        inbound = layer.input\n",
    "        if isinstance(inbound, list):\n",
    "            mapped = [tensor_map.get(x, x) for x in inbound]\n",
    "        else:\n",
    "            mapped = tensor_map.get(inbound, inbound)\n",
    "\n",
    "        x = layer(mapped)\n",
    "\n",
    "        # Insert mask AFTER BatchNorm if previous layer was Conv2D(1x1)\n",
    "        if isinstance(layer, layers.BatchNormalization):\n",
    "            prev = layer.input._keras_history[0]\n",
    "            if isinstance(prev, layers.Conv2D):\n",
    "                if prev.kernel_size == (1, 1) and prev.name in masks:\n",
    "                    x = ChannelMask(\n",
    "                        masks[prev.name],\n",
    "                        alpha=ALPHA,\n",
    "                        name=prev.name + \"_mask\"\n",
    "                    )(x)\n",
    "\n",
    "        tensor_map[layer.output] = x\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=base_model.inputs,\n",
    "        outputs=x,\n",
    "        name=base_model.name + \"_masked\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------- FLOPs ----------------\n",
    "# Count ONLY pointwise convs\n",
    "def baseline_flops(model):\n",
    "    total = 0\n",
    "    for l in find_prunable_convs(model):\n",
    "        if not l._inbound_nodes:\n",
    "            continue\n",
    "        _, h, w, cout = l.output.shape\n",
    "        cin = l.input.shape[-1]\n",
    "        total += h * w * cin * cout * 2\n",
    "    return int(total)\n",
    "\n",
    "\n",
    "def effective_flops_from_masks(model, masks):\n",
    "    total = 0\n",
    "    for l in find_prunable_convs(model):\n",
    "        if l.name in masks and l._inbound_nodes:\n",
    "            _, h, w, _ = l.output.shape\n",
    "            cin = l.input.shape[-1]\n",
    "            active = int(np.sum(masks[l.name]))\n",
    "            total += h * w * cin * active * 2\n",
    "    return int(total)\n",
    "\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "def run():\n",
    "    print(\"[INFO] Loading base MobileNet...\")\n",
    "    base = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "\n",
    "    # force graph build\n",
    "    dummy = tf.zeros((1,) + base.input_shape[1:])\n",
    "    _ = base(dummy)\n",
    "\n",
    "    train, val = load_ds(DATASET_PATH, base.input_shape[1:3])\n",
    "\n",
    "    convs = find_prunable_convs(base)\n",
    "    masks = make_masks(convs)\n",
    "\n",
    "    masked = build_masked_mobilenet(base, masks)\n",
    "\n",
    "    # force masked graph build\n",
    "    _ = masked(dummy)\n",
    "\n",
    "    for l in masked.layers[:-UNFREEZE_LAST]:\n",
    "        l.trainable = False\n",
    "\n",
    "    masked.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    early = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "\n",
    "    masked.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[early],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    masked.save(\"masked_finetuned_mobilenet_Intel.keras\")\n",
    "\n",
    "    b = baseline_flops(base)\n",
    "    e = effective_flops_from_masks(base, masks)\n",
    "\n",
    "    print(\"\\n=========== EFFECTIVE FLOPs ANALYSIS ===========\")\n",
    "    print(f\"Baseline FLOPs : {b:,}\")\n",
    "    print(f\"Effective FLOPs: {e:,}\")\n",
    "    print(f\"Reduction (%)  : {(b - e) / b * 100:.2f}%\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a3d63",
   "metadata": {},
   "source": [
    "# **Main Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f9e01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNIVERSAL TRANSFORMER ATTENTION HEAD PRUNING + MASKED FINE-TUNING\n",
    "(FINAL, GUARANTEED-RUNNING VERSION)\n",
    "\n",
    "âœ” Any CSV or JSON Lines dataset\n",
    "âœ” Label = last column fallback\n",
    "âœ” Labels aligned to model output\n",
    "âœ” Binary + multiclass supported\n",
    "âœ” Fresh TextVectorization\n",
    "âœ” Soft attention-head pruning\n",
    "âœ” Masked fine-tuning (graph-safe)\n",
    "âœ” Correct FLOPs / Effective FLOPs\n",
    "âœ” Handles nested Transformer blocks\n",
    "âœ” Legacy .h5 compatible\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================================\n",
    "# IMPORTS\n",
    "# ==========================================================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIG\n",
    "# ==========================================================\n",
    "\n",
    "MAX_LEN     = 200\n",
    "VOCAB_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 5\n",
    "KEEP_RATIO = 0.7\n",
    "\n",
    "# ==========================================================\n",
    "# CUSTOM LAYERS (LEGACY SAFE)\n",
    "# ==========================================================\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_len=None,\n",
    "        vocab_size=None,\n",
    "        embed_dim=None,\n",
    "        maxlen=None,          # legacy support\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if max_len is None and maxlen is not None:\n",
    "            max_len = maxlen\n",
    "\n",
    "        if max_len is None:\n",
    "            raise ValueError(\"max_len or maxlen must be provided\")\n",
    "\n",
    "        self.max_len = int(max_len)\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.embed_dim = int(embed_dim)\n",
    "\n",
    "        self.token_emb = layers.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.pos_emb   = layers.Embedding(self.max_len, self.embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        pos = tf.range(start=0, limit=tf.shape(x)[-1])\n",
    "        return self.token_emb(x) + self.pos_emb(pos)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"maxlen\": self.max_len,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.drop1 = layers.Dropout(rate)\n",
    "        self.drop2 = layers.Dropout(rate)\n",
    "\n",
    "        self.last_attn = None\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        attn = self.att(x, x, training=training)\n",
    "        self.last_attn = attn\n",
    "        x = self.ln1(x + self.drop1(attn, training=training))\n",
    "        ffn = self.ffn(x, training=training)\n",
    "        return self.ln2(x + self.drop2(ffn, training=training))\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "\n",
    "TransformerBlock = TransformerEncoder\n",
    "\n",
    "# ==========================================================\n",
    "# MASKED TRANSFORMER\n",
    "# ==========================================================\n",
    "\n",
    "class MaskedTransformer(tf.keras.Model):\n",
    "    def __init__(self, base_model, masks):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.masks = masks\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        for layer in self.base.layers:\n",
    "            if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "                continue\n",
    "\n",
    "            if isinstance(layer, TransformerEncoder):\n",
    "                attn = layer.att(x, x, training=training)\n",
    "\n",
    "                if layer in self.masks:\n",
    "                    H = layer.att.num_heads\n",
    "                    D = attn.shape[-1]\n",
    "                    Hd = D // H\n",
    "                    m = self.masks[layer]\n",
    "\n",
    "                    attn = tf.reshape(attn, (-1, tf.shape(attn)[1], H, Hd))\n",
    "                    attn = attn * m[None, None, :, None]\n",
    "                    attn = tf.reshape(attn, (-1, tf.shape(attn)[1], D))\n",
    "\n",
    "                x = layer.ln1(x + layer.drop1(attn, training=training))\n",
    "                x = layer.ln2(x + layer.drop2(layer.ffn(x), training=training))\n",
    "            else:\n",
    "                x = layer(x, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "# ==========================================================\n",
    "# DATASET LOADER (ALIGNED TO MODEL)\n",
    "# ==========================================================\n",
    "\n",
    "def load_text_dataset(path, model_num_outputs):\n",
    "\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    elif path.endswith(\".json\"):\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                rows.append(json.loads(line))\n",
    "        df = pd.DataFrame(rows)\n",
    "    else:\n",
    "        raise ValueError(\"Only CSV and JSON supported\")\n",
    "\n",
    "    print(\"[INFO] Dataset columns:\", list(df.columns))\n",
    "\n",
    "    text_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    texts = df[text_cols].fillna(\"\").agg(\" \".join, axis=1).values\n",
    "\n",
    "    label_col = df.columns[-1]\n",
    "    print(f\"[INFO] Using label column: {label_col}\")\n",
    "\n",
    "    labels = df[label_col].values\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(labels):\n",
    "        y_int = labels.astype(int)\n",
    "    else:\n",
    "        y_int = LabelEncoder().fit_transform(labels)\n",
    "\n",
    "    # ======================================================\n",
    "    # CRITICAL: BINARY vs MULTICLASS HANDLING\n",
    "    # ======================================================\n",
    "\n",
    "    if model_num_outputs == 1:\n",
    "        # Dense(1) â†’ sigmoid â†’ binary\n",
    "        y = y_int.astype(\"float32\")\n",
    "        loss = \"binary_crossentropy\"\n",
    "\n",
    "    else:\n",
    "        # Dense(N) â†’ softmax\n",
    "        y_int = np.clip(y_int, 0, model_num_outputs - 1)\n",
    "        y = to_categorical(y_int, num_classes=model_num_outputs)\n",
    "        loss = \"categorical_crossentropy\"\n",
    "\n",
    "    return texts, y, loss\n",
    "\n",
    "# ==========================================================\n",
    "# TEXT VECTORIZATION\n",
    "# ==========================================================\n",
    "\n",
    "def build_vectorizer(texts):\n",
    "    vec = layers.TextVectorization(\n",
    "        max_tokens=VOCAB_SIZE,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=MAX_LEN\n",
    "    )\n",
    "    vec.adapt(texts)\n",
    "    return vec\n",
    "\n",
    "# ==========================================================\n",
    "# STRIP TRANSFORMER\n",
    "# ==========================================================\n",
    "\n",
    "def strip_transformer(model):\n",
    "    if model.input.dtype in (tf.int32, tf.int64):\n",
    "        return model\n",
    "\n",
    "    for layer in model.layers:\n",
    "        try:\n",
    "            if layer.input.dtype in (tf.int32, tf.int64):\n",
    "                return tf.keras.Model(layer.input, model.output)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    token_input = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
    "    return tf.keras.Model(token_input, model(token_input))\n",
    "\n",
    "# ==========================================================\n",
    "# UTILITIES\n",
    "# ==========================================================\n",
    "\n",
    "def get_all_layers(model):\n",
    "    out = []\n",
    "    for l in model.layers:\n",
    "        out.append(l)\n",
    "        if isinstance(l, tf.keras.Model):\n",
    "            out.extend(get_all_layers(l))\n",
    "    return out\n",
    "\n",
    "# ==========================================================\n",
    "# ATTENTION STATS + FLOPs\n",
    "# ==========================================================\n",
    "\n",
    "def compute_attention_head_stats(model, vectorizer, texts, max_batches=20):\n",
    "    blocks = [l for l in get_all_layers(model) if isinstance(l, TransformerEncoder)]\n",
    "    stats = {b: [] for b in blocks}\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(texts).batch(BATCH_SIZE)\n",
    "    for i, xb in enumerate(ds):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "\n",
    "        tokens = tf.cast(vectorizer(xb), tf.int32)\n",
    "        _ = model(tokens, training=False)\n",
    "\n",
    "        for block in blocks:\n",
    "            if block.last_attn is None:\n",
    "                continue\n",
    "            attn = block.last_attn\n",
    "            H = block.att.num_heads\n",
    "            D = attn.shape[-1]\n",
    "            Hd = D // H\n",
    "            reshaped = tf.reshape(attn, (-1, attn.shape[1], H, Hd))\n",
    "            score = tf.reduce_mean(tf.abs(reshaped), axis=[0, 1, 3]).numpy()\n",
    "            stats[block].append(score)\n",
    "\n",
    "    return {k: np.mean(v, axis=0) for k, v in stats.items()}\n",
    "\n",
    "\n",
    "def compute_importance_mask(stats, keep_ratio):\n",
    "    masks = {}\n",
    "    for block, score in stats.items():\n",
    "        k = max(1, int(len(score) * keep_ratio))\n",
    "        th = np.partition(score, -k)[-k]\n",
    "        masks[block] = (score >= th).astype(np.float32)\n",
    "    return masks\n",
    "\n",
    "\n",
    "def attention_flops(seq_len, embed_dim):\n",
    "    return 4 * seq_len * embed_dim * embed_dim + 2 * seq_len * seq_len * embed_dim\n",
    "\n",
    "\n",
    "def transformer_model_flops(model):\n",
    "    flops = 0\n",
    "    for l in get_all_layers(model):\n",
    "        if isinstance(l, TransformerEncoder):\n",
    "            D = l.att.key_dim * l.att.num_heads\n",
    "            flops += attention_flops(MAX_LEN, D)\n",
    "    return flops\n",
    "\n",
    "\n",
    "def effective_transformer_flops(model, masks):\n",
    "    flops = 0\n",
    "    for l in get_all_layers(model):\n",
    "        if isinstance(l, TransformerEncoder):\n",
    "            D = l.att.key_dim * l.att.num_heads\n",
    "            H = l.att.num_heads\n",
    "            kept = int(np.sum(masks.get(l, np.ones(H))))\n",
    "            flops += attention_flops(MAX_LEN, D) * (kept / H)\n",
    "    return flops\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN PIPELINE\n",
    "# ==========================================================\n",
    "\n",
    "def universal_transformer_pruning(model_path, dataset_path):\n",
    "\n",
    "    model = tf.keras.models.load_model(\n",
    "        model_path,\n",
    "        custom_objects={\n",
    "            \"PositionalEmbedding\": PositionalEmbedding,\n",
    "            \"TransformerEncoder\": TransformerEncoder,\n",
    "            \"TransformerBlock\": TransformerEncoder,\n",
    "        },\n",
    "        compile=False\n",
    "    )\n",
    "\n",
    "    model_num_outputs = model.output_shape[-1]\n",
    "    print(f\"[INFO] Model output units: {model_num_outputs}\")\n",
    "\n",
    "    texts, y, loss_fn = load_text_dataset(dataset_path, model_num_outputs)\n",
    "\n",
    "    Xtr, Xv, ytr, yv = train_test_split(\n",
    "        texts, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y if model_num_outputs == 1 else y.argmax(axis=1)\n",
    "    )\n",
    "\n",
    "    vectorizer = build_vectorizer(Xtr)\n",
    "    transformer = strip_transformer(model)\n",
    "\n",
    "    stats = compute_attention_head_stats(transformer, vectorizer, Xtr)\n",
    "    masks = compute_importance_mask(stats, KEEP_RATIO)\n",
    "\n",
    "    masked_model = MaskedTransformer(transformer, masks)\n",
    "    masked_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=loss_fn,\n",
    "        metrics=[\"accuracy\"],\n",
    "        run_eagerly=True\n",
    "    )\n",
    "\n",
    "    masked_model.fit(\n",
    "        tf.cast(vectorizer(Xtr), tf.int32), ytr,\n",
    "        validation_data=(tf.cast(vectorizer(Xv), tf.int32), yv),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    acc = masked_model.evaluate(\n",
    "        tf.cast(vectorizer(Xv), tf.int32), yv, verbose=0\n",
    "    )[1]\n",
    "\n",
    "    orig = transformer_model_flops(transformer)\n",
    "    eff  = effective_transformer_flops(transformer, masks)\n",
    "\n",
    "    print(\"\\n=========== FINAL RESULTS ===========\")\n",
    "    print(f\"Masked Accuracy     : {acc:.4f}\")\n",
    "    print(f\"Original GFLOPs     : {orig / 1e9:.3f}\")\n",
    "    print(f\"Effective GFLOPs    : {eff / 1e9:.3f}\")\n",
    "    print(f\"FLOPs Reduction (%) : {(1 - eff / orig) * 100:.2f}%\")\n",
    "\n",
    "    masked_model.save(\"soft_pruned_transformer_imbd.keras\")\n",
    "    print(\"[INFO] Saved pruned model: soft_pruned_transformer.keras\")\n",
    "\n",
    "# ==========================================================\n",
    "# RUN\n",
    "# ==========================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    universal_transformer_pruning(\n",
    "        r\"D:\\college\\sem-8\\final\\custom_transformer_imdb.h5\",\n",
    "        r\"D:\\college\\sem-8\\dataset\\tranformer dataset\\IMDB Dataset.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
