{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2514f72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sequential_cnn_pruning_full_fixed.py\n",
    "Full Sequential CNN pruning pipeline (single-file)\n",
    "\n",
    "- Loads user Sequential CNN (.h5/.keras / SavedModel)\n",
    "- Sanitizes layer names in .h5 if they contain '/'\n",
    "- Loads folder dataset (image_dataset_from_directory)\n",
    "- Computes activation/gradient/variance importance\n",
    "- Creates masks (keep_ratio)\n",
    "- Builds masked model (gating layer)\n",
    "- Fine-tunes masked model\n",
    "- Structurally prunes model (conv filter removal, prune Dense outputs only)\n",
    "- Computes FLOPS & timings, evaluates models\n",
    "- Saves models and masks\n",
    "\n",
    "Fixes:\n",
    "- Ensures final layer matches dataset classes (automatically rebuilds final layer if needed)\n",
    "- Uses appropriate loss function (binary vs sparse categorical) during stat collection and training\n",
    "- Adds GFLOPS, accuracy reduction, inference timing\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ---------------------------\n",
    "# USER CONFIG (edit paths)\n",
    "# ---------------------------\n",
    "MODEL_PATH = r\"D:\\college\\sem-8\\models\\garbage_cnn_model.h5\"   # path to model (.h5/.keras or SavedModel dir)\n",
    "DATASET_PATH = r\"D:\\college\\sem-8\\dataset\\Garbage classification\\Garbage classification\"      # folder with subfolders per class\n",
    "SAVE_DIR = r\"pruning_output\"                                 # where outputs are saved\n",
    "base_name = os.path.splitext(os.path.basename(MODEL_PATH))[0]\n",
    "\n",
    "\n",
    "KEEP_RATIO = 0.83            # fraction of channels/units to keep\n",
    "ALPHA, BETA, GAMMA = 0.4, 0.3, 0.3  # importance weights\n",
    "BATCH_SIZE = 64\n",
    "CALIB_BATCHES = 30\n",
    "FT_EPOCHS = 3\n",
    "FT_BATCHES_TO_USE = 150\n",
    "PLOT_RESULTS = True\n",
    "VERBOSE = True\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def log(*args):\n",
    "    if VERBOSE:\n",
    "        print(\"[INFO]\", *args)\n",
    "\n",
    "# ---------------------------\n",
    "# Safe load model (handles '/' in h5 layer names)\n",
    "# ---------------------------\n",
    "def safe_load_model(model_path):\n",
    "    \"\"\"\n",
    "    Try normal load_model; if fails (e.g., '/' in layer names in H5), sanitize model_config JSON and rebuild.\n",
    "    Returns a Keras model (compiled=False).\n",
    "    \"\"\"\n",
    "    log(\"Loading model:\", model_path)\n",
    "    # Try direct load first\n",
    "    try:\n",
    "        m = tf.keras.models.load_model(model_path, compile=False)\n",
    "        log(\"Loaded model directly.\")\n",
    "        return m\n",
    "    except Exception as e:\n",
    "        log(\"Direct load failed:\", e)\n",
    "\n",
    "    # If HDF5, attempt to sanitize layer names in model_config\n",
    "    try:\n",
    "        with h5py.File(model_path, \"r\") as f:\n",
    "            if \"model_config\" in f:\n",
    "                raw = f[\"model_config\"][()]\n",
    "                if isinstance(raw, bytes):\n",
    "                    raw = raw.decode(\"utf-8\")\n",
    "                cfg_json = json.loads(raw)\n",
    "                changed = False\n",
    "                for layer in cfg_json.get(\"config\", {}).get(\"layers\", []):\n",
    "                    cfg = layer.get(\"config\", {})\n",
    "                    name = cfg.get(\"name\")\n",
    "                    if isinstance(name, str) and \"/\" in name:\n",
    "                        new_name = name.replace(\"/\", \"_\")\n",
    "                        cfg[\"name\"] = new_name\n",
    "                        changed = True\n",
    "                        log(f\"[FIX] layer name: {name} -> {new_name}\")\n",
    "                if changed:\n",
    "                    fixed_json = json.dumps(cfg_json)\n",
    "                    model = tf.keras.models.model_from_json(fixed_json)\n",
    "                    model.load_weights(model_path)\n",
    "                    log(\"Loaded model from sanitized JSON + weights.\")\n",
    "                    return model\n",
    "    except Exception as e2:\n",
    "        log(\"H5 sanitization attempt failed:\", e2)\n",
    "\n",
    "    # final fallback: try load_model with safe_mode=False (older TF)\n",
    "    try:\n",
    "        m = tf.keras.models.load_model(model_path, compile=False, safe_mode=False)\n",
    "        log(\"Loaded model with safe_mode=False.\")\n",
    "        return m\n",
    "    except Exception as e3:\n",
    "        log(\"All load attempts failed:\", e3)\n",
    "        raise RuntimeError(\"Failed to load model. Ensure path and format are correct.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Rebuild final layer to match num_classes (safe, best-effort)\n",
    "# ---------------------------\n",
    "def ensure_output_matches_dataset(orig_model, num_classes):\n",
    "    \"\"\"\n",
    "    If the model's current output shape doesn't match num_classes, rebuild final output\n",
    "    to match. Works for Sequential-style networks. Returns new_model, loss_fn.\n",
    "    \"\"\"\n",
    "    # Determine model's output dim\n",
    "    out_shape = tuple(orig_model.output_shape) if orig_model.output_shape is not None else None\n",
    "    # If already matches (and for multiclass softmax case), return original and appropriate loss.\n",
    "    if out_shape is not None:\n",
    "        if num_classes == 2 and (out_shape[-1] == 1 or out_shape[-1] == 2):\n",
    "            # binary case: allow Dense(1) or Dense(2) (Dense(2) could be softmax but labels are 0/1)\n",
    "            log(\"Model output seems compatible with binary classification.\")\n",
    "            loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False) if out_shape[-1] == 1 else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "            return orig_model, loss_fn\n",
    "        if num_classes > 2 and out_shape[-1] == num_classes:\n",
    "            log(\"Model output matches dataset classes.\")\n",
    "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "            return orig_model, loss_fn\n",
    "\n",
    "    # Need to rebuild final layer\n",
    "    log(\"Rebuilding model final layer to match num_classes:\", num_classes)\n",
    "    # We'll clone all layers except the last one and then append a new final dense.\n",
    "    # Use layer.from_config to avoid reusing layer objects in two models.\n",
    "    new_seq = models.Sequential(name=(orig_model.name or \"rebuilt_model\") + \"_rebuilt\")\n",
    "    # Add InputLayer\n",
    "    input_shape = orig_model.input_shape[1:]\n",
    "    new_seq.add(layers.InputLayer(input_shape=tuple(input_shape)))\n",
    "\n",
    "    # Clone all layers except the last (we will replace last)\n",
    "    # We'll attempt to copy weights for layers that remain identical\n",
    "    for layer in orig_model.layers[:-1]:\n",
    "        try:\n",
    "            cfg = layer.get_config()\n",
    "            Cls = layer.__class__\n",
    "            cloned = Cls.from_config(cfg)\n",
    "            new_seq.add(cloned)\n",
    "            # set weights if possible and shapes match\n",
    "            try:\n",
    "                w = layer.get_weights()\n",
    "                if w:\n",
    "                    new_seq.layers[-1].set_weights(w)\n",
    "            except Exception:\n",
    "                # ignore weight copy failures\n",
    "                pass\n",
    "        except Exception:\n",
    "            # fallback: try to append original layer (may cause errors but best-effort)\n",
    "            try:\n",
    "                new_seq.add(layer)\n",
    "            except Exception:\n",
    "                log(\"Warning: couldn't clone layer\", layer.name, \"- skipping weights copy.\")\n",
    "\n",
    "    # Add new final layer depending on num_classes\n",
    "    if num_classes == 2:\n",
    "        # binary: Dense(1, activation='sigmoid')\n",
    "        new_seq.add(layers.Dense(1, activation=\"sigmoid\", name=\"output_rebuilt\"))\n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    else:\n",
    "        new_seq.add(layers.Dense(num_classes, activation=\"softmax\", name=\"output_rebuilt\"))\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    # Try to compile minimal to ensure shape correctness (we'll let caller compile fully later)\n",
    "    return new_seq, loss_fn\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset loader (folder-based)\n",
    "# ---------------------------\n",
    "def load_image_folder_dataset(path, image_size, batch_size=BATCH_SIZE):\n",
    "    log(\"Loading dataset folder:\", path, \"image_size:\", image_size)\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=42,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=42,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    rescaler = layers.Rescaling(1.0 / 255)\n",
    "    train_ds = train_ds.map(lambda x, y: (rescaler(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(lambda x, y: (rescaler(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "# ---------------------------\n",
    "# Activation & gradient stats (uses provided loss function)\n",
    "# ---------------------------\n",
    "def compute_activation_grad_stats(model, layer_names, dataset, loss_fn, max_batches=CALIB_BATCHES):\n",
    "    \"\"\"\n",
    "    For each layer in layer_names, compute:\n",
    "      - A: mean(abs(activation)) per filter/unit\n",
    "      - G: mean(abs(grad wrt activation)) per filter/unit\n",
    "      - V: variance(activation) per filter/unit\n",
    "    Returns: dict name -> (A, G, V)\n",
    "    \"\"\"\n",
    "    log(\"Computing activation & gradient stats...\")\n",
    "    results = {n: [] for n in layer_names}\n",
    "    grad_results = {n: [] for n in layer_names}\n",
    "    var_results = {n: [] for n in layer_names}\n",
    "\n",
    "    batch_count = 0\n",
    "    for x_batch, y_batch in dataset:\n",
    "        if batch_count >= max_batches:\n",
    "            break\n",
    "        batch_count += 1\n",
    "        layer_acts = {}\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            a = x_batch\n",
    "            for layer in model.layers:\n",
    "                a = layer(a)\n",
    "                if layer.name in layer_names:\n",
    "                    tape.watch(a)\n",
    "                    layer_acts[layer.name] = a\n",
    "            preds = a\n",
    "            # compute loss using provided loss_fn (works for both binary & sparse categorical)\n",
    "            # When loss_fn is a Keras loss instance, call like a function\n",
    "            # Ensure y_batch dtype is int for sparse CE, float for binary BCE\n",
    "            try:\n",
    "                loss_vals = loss_fn(y_batch, preds)\n",
    "            except Exception:\n",
    "                # fallback attempt: convert types for binary\n",
    "                if isinstance(loss_fn, tf.keras.losses.BinaryCrossentropy):\n",
    "                    loss_vals = loss_fn(tf.cast(y_batch, tf.float32), preds)\n",
    "                else:\n",
    "                    loss_vals = loss_fn(y_batch, preds)\n",
    "            loss = tf.reduce_mean(loss_vals)\n",
    "\n",
    "        for name in layer_names:\n",
    "            a = layer_acts[name]\n",
    "            if len(a.shape) == 4:\n",
    "                A = tf.reduce_mean(tf.abs(a), axis=(0,1,2)).numpy()\n",
    "                V = tf.math.reduce_variance(a, axis=(0,1,2)).numpy()\n",
    "            else:\n",
    "                A = tf.reduce_mean(tf.abs(a), axis=0).numpy()\n",
    "                V = tf.math.reduce_variance(a, axis=0).numpy()\n",
    "\n",
    "            grad = tape.gradient(loss, a)\n",
    "            if grad is None:\n",
    "                G = np.zeros_like(A)\n",
    "            else:\n",
    "                if len(grad.shape) == 4:\n",
    "                    G = tf.reduce_mean(tf.abs(grad), axis=(0,1,2)).numpy()\n",
    "                else:\n",
    "                    G = tf.reduce_mean(tf.abs(grad), axis=0).numpy()\n",
    "\n",
    "            results[name].append(A)\n",
    "            var_results[name].append(V)\n",
    "            grad_results[name].append(G)\n",
    "        del tape\n",
    "\n",
    "    stats = {}\n",
    "    for name in layer_names:\n",
    "        A = np.mean(results[name], axis=0)\n",
    "        V = np.mean(var_results[name], axis=0)\n",
    "        G = np.mean(grad_results[name], axis=0)\n",
    "        stats[name] = (A, G, V)\n",
    "        log(f\"{name}: len={len(A)} meanA={A.mean():.6e} meanG={G.mean():.6e} meanV={V.mean():.6e}\")\n",
    "    return stats\n",
    "\n",
    "# ---------------------------\n",
    "# Importance scores & masks\n",
    "# ---------------------------\n",
    "def compute_importance_scores(stats, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n",
    "    def normalize(x):\n",
    "        x = x - x.min()\n",
    "        if x.max() > 0:\n",
    "            x = x / x.max()\n",
    "        return x\n",
    "    scores = {}\n",
    "    for name, (A, G, V) in stats.items():\n",
    "        scores[name] = alpha * normalize(A) + beta * normalize(G) + gamma * normalize(V)\n",
    "    return scores\n",
    "\n",
    "def make_masks_from_scores(score_map, keep_ratio=KEEP_RATIO):\n",
    "    masks = {}\n",
    "    for name, score in score_map.items():\n",
    "        k = max(1, int(len(score) * keep_ratio))\n",
    "        thresh = np.partition(score, -k)[-k]\n",
    "        mask = (score >= thresh).astype(np.float32)\n",
    "        masks[name] = mask\n",
    "        log(f\"{name}: keep {int(mask.sum())}/{len(mask)}\")\n",
    "    return masks\n",
    "\n",
    "# ---------------------------\n",
    "# Mask gate layer & masked model\n",
    "# ---------------------------\n",
    "class CNNGate(tf.keras.layers.Layer):\n",
    "    def __init__(self, channels, mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        channels: int number of channels this gate controls\n",
    "        mask: 1D array-like of length 'channels' with 0/1 values (or floats in [0,1]).\n",
    "              If provided, gate variable is initialized to these values; otherwise ones.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.channels = int(channels)\n",
    "        # store mask as numpy array if provided (for serialization convenience)\n",
    "        self._init_mask = None if mask is None else np.array(mask, dtype=np.float32)\n",
    "        init_val = self._init_mask if self._init_mask is not None else np.ones((self.channels,), dtype=np.float32)\n",
    "        # gate is non-trainable scalar per-channel multiplier\n",
    "        self.gate = self.add_weight(\n",
    "            name=\"gate\",\n",
    "            shape=(self.channels,),\n",
    "            initializer=tf.keras.initializers.Constant(init_val),\n",
    "            trainable=False,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # supports inputs with shape [batch, H, W, C] or [batch, C] (works broadcasting)\n",
    "        g = self.gate\n",
    "        # expand dims to match channels in conv output\n",
    "        if len(inputs.shape) == 4:\n",
    "            return inputs * g[None, None, None, :]\n",
    "        elif len(inputs.shape) == 2:\n",
    "            return inputs * g[None, :]\n",
    "        else:\n",
    "            # fallback broadcasting\n",
    "            return inputs * g\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        # do NOT embed gate values directly here (weights are saved by Keras),\n",
    "        # but we include channels for reconstruction convenience.\n",
    "        cfg.update({\n",
    "            \"channels\": self.channels,\n",
    "            # don't include mask/gate here to avoid duplicating weight data;\n",
    "            # the gate weight will be saved/loaded normally by Keras.\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # config may only contain \"channels\" — weight values will be restored by Keras load.\n",
    "        channels = config.pop(\"channels\")\n",
    "        return cls(channels=channels, **config)\n",
    "\n",
    "\n",
    "def build_masked_model(orig_model, masks):\n",
    "    \"\"\"\n",
    "    Build a new functional model with cloned layers from orig_model and insert a CNNGate\n",
    "    after each layer named in `masks`. The gate is initialized from masks[layer_name].\n",
    "    \"\"\"\n",
    "    log(\"Building masked model with gates (cloning layers)...\")\n",
    "    inp = tf.keras.Input(shape=orig_model.input_shape[1:])\n",
    "    x = inp\n",
    "\n",
    "    # keep mapping from original layer name -> new layer object (for weight copying)\n",
    "    for layer in orig_model.layers:\n",
    "        # clone layer if possible to avoid reusing original layer objects\n",
    "        try:\n",
    "            cfg = layer.get_config()\n",
    "            Cls = layer.__class__\n",
    "            new_layer = Cls.from_config(cfg)\n",
    "        except Exception:\n",
    "            # fallback: try to reuse the layer (less safe)\n",
    "            new_layer = layer\n",
    "\n",
    "        # apply the new layer to current tensor\n",
    "        x = new_layer(x)\n",
    "\n",
    "        # copy weights if layer had weights and we cloned it\n",
    "        try:\n",
    "            w = layer.get_weights()\n",
    "            if w:\n",
    "                try:\n",
    "                    new_layer.set_weights(w)\n",
    "                except Exception:\n",
    "                    # some layers (e.g., fused ops) may not accept direct set_weights — ignore\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # insert gate if this original layer is in masks\n",
    "        if layer.name in masks:\n",
    "            mask = np.array(masks[layer.name], dtype=np.float32)\n",
    "            channels = int(mask.shape[0])\n",
    "            gate_layer = CNNGate(channels=channels, mask=mask, name=layer.name + \"_gate\")\n",
    "            x = gate_layer(x)\n",
    "            # gate weight is already initialized from mask in CNNGate.__init__, so nothing else to do\n",
    "\n",
    "    masked = tf.keras.Model(inputs=inp, outputs=x, name=(orig_model.name or \"model\") + \"_masked\")\n",
    "    log(\"Masked model created:\", masked.name)\n",
    "    return masked\n",
    "\n",
    "# ---------------------------\n",
    "# Structural pruning (safe)\n",
    "# ---------------------------\n",
    "def prune_structural_sequential(orig_model, masks, input_shape):\n",
    "    \"\"\"\n",
    "    Structural pruning for Sequential models.\n",
    "    - Prune Conv2D output filters using masks[layer.name]\n",
    "    - Prune Dense output units only (do not slice Dense input rows that come from Flatten/Conv)\n",
    "    - Attempt to slice BatchNorm params to match conv outputs\n",
    "    \"\"\"\n",
    "    log(\"Structural pruning (safe) start...\")\n",
    "    new_layers = []\n",
    "    prev_was_conv_like = False  # indicates that Flatten/Conv preceded Dense inputs\n",
    "\n",
    "    for layer in orig_model.layers:\n",
    "        # Conv2D: slice output channels\n",
    "        if isinstance(layer, layers.Conv2D):\n",
    "            W, b = layer.get_weights()\n",
    "            orig_out = W.shape[-1]\n",
    "            mask = masks.get(layer.name, np.ones(orig_out, dtype=np.float32))\n",
    "            keep_idx = np.where(mask == 1)[0]\n",
    "            if keep_idx.size == 0:\n",
    "                keep_idx = np.array([int(np.argmax(mask))], dtype=np.int32)\n",
    "            W_new = W[:, :, :, keep_idx]\n",
    "            b_new = b[keep_idx]\n",
    "            new_conv = layers.Conv2D(\n",
    "                filters=len(keep_idx),\n",
    "                kernel_size=layer.kernel_size,\n",
    "                strides=layer.strides,\n",
    "                padding=layer.padding,\n",
    "                activation=layer.activation,\n",
    "                use_bias=layer.use_bias,\n",
    "                name=layer.name + \"_pruned\"\n",
    "            )\n",
    "            new_layers.append((new_conv, [W_new, b_new]))\n",
    "            prev_was_conv_like = True\n",
    "            continue\n",
    "\n",
    "        # BatchNorm: slice params if prev was conv-like\n",
    "        if isinstance(layer, layers.BatchNormalization):\n",
    "            try:\n",
    "                weights = layer.get_weights()\n",
    "                if prev_was_conv_like and new_layers:\n",
    "                    prev_layer_obj, prev_w = new_layers[-1]\n",
    "                    if prev_w is not None:\n",
    "                        out_ch = prev_w[0].shape[-1]  # kernel last dim\n",
    "                        gamma, beta, mean, var = weights\n",
    "                        gamma = gamma[:out_ch]\n",
    "                        beta = beta[:out_ch]\n",
    "                        mean = mean[:out_ch]\n",
    "                        var = var[:out_ch]\n",
    "                        new_bn = layers.BatchNormalization.from_config(layer.get_config())\n",
    "                        new_layers.append((new_bn, [gamma, beta, mean, var]))\n",
    "                        continue\n",
    "            except Exception:\n",
    "                pass\n",
    "            # fallback keep BN as-is\n",
    "            try:\n",
    "                new_bn = layers.BatchNormalization.from_config(layer.get_config())\n",
    "                new_layers.append((new_bn, layer.get_weights()))\n",
    "            except Exception:\n",
    "                new_layers.append((layer, layer.get_weights() if hasattr(layer, \"get_weights\") else None))\n",
    "            # prev_was_conv_like unchanged\n",
    "            continue\n",
    "\n",
    "        # MaxPool/Activation/Dropout/Flatten/GlobalAvgPool: clone or reuse\n",
    "        if isinstance(layer, (layers.MaxPooling2D, layers.Activation, layers.ReLU, layers.Dropout, layers.Flatten, layers.GlobalAveragePooling2D)):\n",
    "            try:\n",
    "                cloned = layer.__class__.from_config(layer.get_config())\n",
    "                w = layer.get_weights() if hasattr(layer, \"get_weights\") else None\n",
    "                new_layers.append((cloned, w if w else None))\n",
    "            except Exception:\n",
    "                new_layers.append((layer, layer.get_weights() if hasattr(layer, \"get_weights\") else None))\n",
    "            if isinstance(layer, layers.Flatten) or isinstance(layer, layers.GlobalAveragePooling2D):\n",
    "                prev_was_conv_like = True\n",
    "            continue\n",
    "\n",
    "        # Dense: prune outputs only (safe)\n",
    "        if isinstance(layer, layers.Dense):\n",
    "            W, b = layer.get_weights()  # shape (in_dim, out_dim)\n",
    "            out_mask = masks.get(layer.name, np.ones(W.shape[1], dtype=np.float32))\n",
    "            out_idx = np.where(out_mask == 1)[0]\n",
    "            if out_idx.size == 0:\n",
    "                out_idx = np.array([int(np.argmax(out_mask))], dtype=np.int32)\n",
    "            W_new = W[:, out_idx]   # keep all input rows (safe)\n",
    "            b_new = b[out_idx]\n",
    "            new_dense = layers.Dense(units=W_new.shape[1], activation=layer.activation, name=layer.name + \"_pruned\")\n",
    "            new_layers.append((new_dense, [W_new, b_new]))\n",
    "            prev_was_conv_like = False\n",
    "            continue\n",
    "\n",
    "        # Fallback for other layers\n",
    "        try:\n",
    "            cloned = layer.__class__.from_config(layer.get_config())\n",
    "            w = layer.get_weights() if hasattr(layer, \"get_weights\") else None\n",
    "            new_layers.append((cloned, w if w else None))\n",
    "        except Exception:\n",
    "            new_layers.append((layer, layer.get_weights() if hasattr(layer, \"get_weights\") else None))\n",
    "        prev_was_conv_like = False\n",
    "\n",
    "    # Build new Sequential model with InputLayer\n",
    "    seq = models.Sequential(name=orig_model.name + \"_struct_pruned\")\n",
    "    seq.add(layers.InputLayer(input_shape=tuple(input_shape)))\n",
    "    for lyr_obj, w in new_layers:\n",
    "        seq.add(lyr_obj)\n",
    "        if w is not None:\n",
    "            try:\n",
    "                seq.layers[-1].set_weights(w)\n",
    "            except Exception as e:\n",
    "                log(\"Warning: couldn't set weights for\", seq.layers[-1].name, \":\", e)\n",
    "    log(\"Structural pruning complete. New model summary:\")\n",
    "    seq.summary()\n",
    "    return seq\n",
    "\n",
    "# ---------------------------\n",
    "# FLOPS and timing helpers\n",
    "# ---------------------------\n",
    "def calculate_conv_flops(input_shape, kernel_shape, strides=(1,1), padding='same'):\n",
    "    h_in, w_in, c_in = input_shape\n",
    "    kh, kw, _, c_out = kernel_shape\n",
    "    if padding == 'same':\n",
    "        h_out = math.ceil(h_in / strides[0])\n",
    "        w_out = math.ceil(w_in / strides[1])\n",
    "    else:\n",
    "        h_out = math.ceil((h_in - kh + 1) / strides[0])\n",
    "        w_out = math.ceil((w_in - kw + 1) / strides[1])\n",
    "    flops = h_out * w_out * (kh * kw * c_in) * c_out * 2\n",
    "    return flops, (h_out, w_out, c_out)\n",
    "\n",
    "def calculate_dense_flops(in_size, out_size):\n",
    "    return in_size * out_size * 2\n",
    "\n",
    "def calculate_model_flops(model, input_shape):\n",
    "    total = 0\n",
    "    current_shape = tuple(input_shape)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, layers.Conv2D):\n",
    "            weights = layer.get_weights()\n",
    "            if not weights:\n",
    "                continue\n",
    "            kernel_shape = weights[0].shape  # (kh, kw, in_c, out_c)\n",
    "            layer_flops, current_shape = calculate_conv_flops(current_shape, kernel_shape, strides=layer.strides, padding=layer.padding)\n",
    "            total += layer_flops\n",
    "        elif isinstance(layer, layers.Flatten):\n",
    "            current_shape = (int(np.prod(current_shape)),)\n",
    "        elif isinstance(layer, layers.Dense):\n",
    "            in_size = current_shape[0] if isinstance(current_shape, tuple) and len(current_shape)>0 else int(current_shape)\n",
    "            layer_flops = calculate_dense_flops(in_size, layer.units)\n",
    "            total += layer_flops\n",
    "            current_shape = (layer.units,)\n",
    "        elif isinstance(layer, layers.MaxPooling2D):\n",
    "            h,w,c = current_shape\n",
    "            pool = layer.pool_size[0] if hasattr(layer.pool_size, \"__getitem__\") else layer.pool_size\n",
    "            current_shape = (h//pool, w//pool, c)\n",
    "        else:\n",
    "            # ignore other layers for shape changes\n",
    "            pass\n",
    "    return total\n",
    "\n",
    "def measure_inference_time(model, sample_batch, steps=20):\n",
    "    model.predict(sample_batch, verbose=0)  # warmup\n",
    "    t0 = time.time()\n",
    "    for _ in range(steps):\n",
    "        model.predict(sample_batch, verbose=0)\n",
    "    t1 = time.time()\n",
    "    return (t1 - t0) / steps\n",
    "\n",
    "# ---------------------------\n",
    "# Save masks helper\n",
    "# ---------------------------\n",
    "def save_masks(masks, path):\n",
    "    serial = {k: v.tolist() for k,v in masks.items()}\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(serial, f, indent=2)\n",
    "    log(\"Saved masks to\", path)\n",
    "\n",
    "# ---------------------------\n",
    "# Plot helper\n",
    "# ---------------------------\n",
    "def plot_mask_histograms(masks, outdir=SAVE_DIR):\n",
    "    if not PLOT_RESULTS:\n",
    "        return\n",
    "    for name, mask in masks.items():\n",
    "        plt.figure(figsize=(5,2))\n",
    "        plt.title(name)\n",
    "        plt.hist(mask, bins=2)\n",
    "        plt.xlabel(\"0=pruned, 1=kept\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"mask_{name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN pipeline\n",
    "# ---------------------------\n",
    "def full_pipeline(model_path, dataset_path):\n",
    "    # 1) load model (safe)\n",
    "    model = safe_load_model(model_path)\n",
    "    log(\"Model loaded. Summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    # 2) load dataset and infer num_classes\n",
    "    # We need input size so infer from model\n",
    "    input_shape = model.input_shape[1:]\n",
    "    log(\"Inferred input shape:\", input_shape)\n",
    "\n",
    "    train_ds, val_ds = load_image_folder_dataset(dataset_path, image_size=input_shape[:2], batch_size=BATCH_SIZE)\n",
    "    # Determine number of classes from dataset\n",
    "    try:\n",
    "        # tf.data.Dataset from image_dataset_from_directory has .class_names on the original Dataset returned object,\n",
    "        # but not on the batched dataset — so inspect via a fresh loader:\n",
    "        tmp = tf.keras.utils.image_dataset_from_directory(dataset_path, image_size=input_shape[:2], batch_size=1)\n",
    "        num_classes = len(tmp.class_names)\n",
    "        del tmp\n",
    "    except Exception:\n",
    "        # fallback: infer from labels in train_ds\n",
    "        classes = set()\n",
    "        for _, y in train_ds.take(10):\n",
    "            classes.update(y.numpy().tolist())\n",
    "        num_classes = max(classes) + 1 if classes else 2\n",
    "\n",
    "    log(\"Detected dataset classes (num_classes):\", num_classes)\n",
    "\n",
    "    # 3) ensure model output matches dataset classes\n",
    "    model, loss_fn = ensure_output_matches_dataset(model, num_classes)\n",
    "    # compile original so evaluate works (use small lr default) with detected loss\n",
    "    if isinstance(loss_fn, tf.keras.losses.BinaryCrossentropy):\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_fn, metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_fn, metrics=[\"accuracy\"])\n",
    "\n",
    "    # reprint summary\n",
    "    log(\"Final model used (after potential rebuild). Summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    # calibration subset for stats\n",
    "    calib_ds = train_ds.take(CALIB_BATCHES)\n",
    "\n",
    "    # sample for timing\n",
    "    try:\n",
    "        sample_x, _ = next(iter(val_ds))\n",
    "    except Exception:\n",
    "        sample_x, _ = next(iter(train_ds))\n",
    "    sample_x_small = sample_x[:min(16, sample_x.shape[0])]\n",
    "\n",
    "    # 4) choose layers to prune (conv + hidden dense only, not final output)\n",
    "    dense_layers = [lyr for lyr in model.layers if isinstance(lyr, layers.Dense)]\n",
    "    last_dense = dense_layers[-1] if dense_layers else None\n",
    "\n",
    "    prune_layer_names = []\n",
    "    for lyr in model.layers:\n",
    "        if isinstance(lyr, layers.Conv2D):\n",
    "            prune_layer_names.append(lyr.name)\n",
    "        elif isinstance(lyr, layers.Dense) and lyr is not last_dense:\n",
    "            prune_layer_names.append(lyr.name)\n",
    "    log(\"Layers considered for pruning:\", prune_layer_names)\n",
    "\n",
    "    # 5) compute stats (pass loss_fn)\n",
    "    stats = compute_activation_grad_stats(model, prune_layer_names, calib_ds, loss_fn=loss_fn, max_batches=CALIB_BATCHES)\n",
    "\n",
    "    # 6) importance & masks\n",
    "    score_map = compute_importance_scores(stats)\n",
    "    masks = make_masks_from_scores(score_map, keep_ratio=KEEP_RATIO)\n",
    "    save_masks(masks, os.path.join(SAVE_DIR, \"masks.json\"))\n",
    "    plot_mask_histograms(masks)\n",
    "\n",
    "    # 7) build masked model and compile (use same loss)\n",
    "    masked_model = build_masked_model(model, masks)\n",
    "    masked_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=loss_fn, metrics=[\"accuracy\"])\n",
    "    log(\"Masked model built.\")\n",
    "\n",
    "    # quick eval before FT (small subset)\n",
    "        # quick eval before FT (small subset)\n",
    "    try:\n",
    "        loss0, acc0 = masked_model.evaluate(val_ds.take(5), verbose=0)\n",
    "        log(\"Masked model pre-FT acc:\", acc0)\n",
    "    except Exception as e:\n",
    "        log(\"Masked model pre-eval failed:\", e)\n",
    "        acc0 = None\n",
    "\n",
    "    # 8) measure baseline flops & time\n",
    "    baseline_flops = calculate_model_flops(model, input_shape)\n",
    "    baseline_time = measure_inference_time(model, sample_x_small, steps=10)\n",
    "    log(f\"Baseline FLOPS: {baseline_flops:,}, baseline time (avg batch): {baseline_time:.4f}s\")\n",
    "\n",
    "    # 9) fine-tune masked model\n",
    "    try:\n",
    "        masked_model.fit(\n",
    "            train_ds.take(FT_BATCHES_TO_USE),\n",
    "            validation_data=val_ds.take(5),\n",
    "            epochs=FT_EPOCHS,\n",
    "            verbose=2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log(\"Masked fine-tune failed/partial:\", e)\n",
    "\n",
    "    # 10) structural prune\n",
    "    # NOTE: you can prune the original model or the masked_model.\n",
    "    #       pruning masked_model preserves gating decisions — often desired.\n",
    "    try:\n",
    "        pruned_model = prune_structural_sequential(masked_model, masks, input_shape)\n",
    "    except Exception:\n",
    "        # fallback to pruning original model if pruning masked_model fails\n",
    "        pruned_model = prune_structural_sequential(model, masks, input_shape)\n",
    "\n",
    "    # compile pruned model for evaluation / training with same loss\n",
    "    pruned_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=loss_fn,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    log(\"Starting fine-tuning of PRUNED model (EarlyStopping)...\")\n",
    "    early_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=2,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    try:\n",
    "        pruned_model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=20,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[early_cb],\n",
    "            verbose=2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log(\"Pruned model fine-tune failed/partial:\", e)\n",
    "\n",
    "    # 11) evaluate models\n",
    "    log(\"Evaluating Original model:\")\n",
    "    orig_loss, orig_acc = model.evaluate(val_ds, verbose=0)\n",
    "    log(\"Original acc:\", orig_acc)\n",
    "\n",
    "    log(\"Evaluating Masked model (after FT):\")\n",
    "    try:\n",
    "        mask_loss, mask_acc = masked_model.evaluate(val_ds, verbose=0)\n",
    "        log(\"Masked acc:\", mask_acc)\n",
    "    except Exception as e:\n",
    "        log(\"Masked evaluate failed:\", e)\n",
    "        mask_acc = None\n",
    "\n",
    "    log(\"Evaluating Pruned model:\")\n",
    "    try:\n",
    "        pruned_loss, pruned_acc = pruned_model.evaluate(val_ds, verbose=0)\n",
    "        log(\"Pruned acc:\", pruned_acc)\n",
    "    except Exception as e:\n",
    "        log(\"Pruned evaluate failed:\", e)\n",
    "        pruned_acc = None\n",
    "\n",
    "    # 12) FLOPS & timing after prune\n",
    "    pruned_flops = calculate_model_flops(pruned_model, input_shape)\n",
    "    pruned_time = measure_inference_time(pruned_model, sample_x_small, steps=10)\n",
    "    log(f\"Pruned FLOPS: {pruned_flops:,}, pruned time: {pruned_time:.4f}s\")\n",
    "\n",
    "    # 13) summary & save\n",
    "    reduction = 1.0 - (pruned_flops / baseline_flops) if baseline_flops > 0 else 0.0\n",
    "    log(\"=\"*60)\n",
    "    log(\"SUMMARY:\")\n",
    "    log(f\"Baseline FLOPS: {baseline_flops:,}\")\n",
    "    log(f\"Pruned FLOPS: {pruned_flops:,}\")\n",
    "    log(f\"FLOPS reduction: {reduction:.2%}\")\n",
    "    log(f\"Original acc: {orig_acc}, Masked acc: {mask_acc}, Pruned acc: {pruned_acc}\")\n",
    "    log(\"=\"*60)\n",
    "\n",
    "    # ---- Extra Metrics: GFLOPS + Accuracy Reduction ----\n",
    "    baseline_gflops = baseline_flops / 1e9\n",
    "    pruned_gflops = pruned_flops / 1e9\n",
    "    gflops_reduction = 1.0 - (pruned_gflops / baseline_gflops) if baseline_gflops > 0 else 0.0\n",
    "    acc_reduction = (orig_acc - pruned_acc) if (orig_acc is not None and pruned_acc is not None) else None\n",
    "\n",
    "    log(f\"Baseline GFLOPS: {baseline_gflops:.4f}\")\n",
    "    log(f\"Pruned GFLOPS: {pruned_gflops:.4f}\")\n",
    "    log(f\"GFLOPS reduction: {gflops_reduction:.2%}\")\n",
    "    if acc_reduction is not None:\n",
    "        log(f\"Accuracy reduction: {acc_reduction:.4f}\")\n",
    "    else:\n",
    "        log(\"Accuracy reduction: N/A\")\n",
    "\n",
    "    # save artifacts\n",
    "    try:\n",
    "        # use proper file extensions to avoid Keras save errors\n",
    "        baseline_name = base_name + \"_baseline.keras\"\n",
    "        masked_name   = base_name + \"_masked.keras\"\n",
    "        pruned_name   = base_name + \"_pruned.keras\"\n",
    "\n",
    "        model.save(os.path.join(SAVE_DIR, baseline_name))\n",
    "        masked_model.save(os.path.join(SAVE_DIR, masked_name))\n",
    "        pruned_model.save(os.path.join(SAVE_DIR, pruned_name))\n",
    "\n",
    "        log(f\"Saved models under names: {baseline_name}, {masked_name}, {pruned_name}\")\n",
    "\n",
    "        log(\"Saved models to\", SAVE_DIR)\n",
    "    except Exception as e:\n",
    "        log(\"Save models failed:\", e)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"masked_model\": masked_model,\n",
    "        \"pruned_model\": pruned_model,\n",
    "        \"masks\": masks,\n",
    "        \"baseline_flops\": baseline_flops,\n",
    "        \"pruned_flops\": pruned_flops,\n",
    "        \"baseline_time\": baseline_time,\n",
    "        \"pruned_time\": pruned_time,\n",
    "        \"orig_acc\": orig_acc,\n",
    "        \"mask_acc\": mask_acc,\n",
    "        \"pruned_acc\": pruned_acc\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Run\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    out = full_pipeline(MODEL_PATH, DATASET_PATH)\n",
    "    log(\"Pipeline finished. Outputs:\", out.keys())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
